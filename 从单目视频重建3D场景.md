# 从单目视频重建3D场景

![image-20250330104047423](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250330104047423.png)

![image-20250330104112468](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250330104112468.png)

从 MegaSaM 的论文来看，它主要是针对 **动态场景下的相机位姿估计和深度重建** 进行优化，而 ORB-SLAM3 和 COLMAP 主要是为 **静态场景** 设计的。

### **理解传统视觉 SLAM 系统的工作原理**

ORB-SLAM3 是一种成熟的传统视觉 SLAM 系统，它主要处理静态场景，并且具有 **良好的实时性** 和 **高精度**，特别是在 **大视差** 和 **清晰场景** 下。学习 **ORB-SLAM3** 的工作原理，尤其是 **特征提取**、**匹配**、**地图构建** 和 **捆绑调整（BA）**，能够为理解 **MegaSaM** 提供很好的基础。ORB-SLAM3的核心部分是 **相机位姿估计** 和 **优化过程**，这对学习 MegaSaM 中的 **深度学习增强的视觉 SLAM** 有很大的启发作用.在学习 **ORB-SLAM3** 时，将接触到 **局部地图优化** 和 **全局捆绑调整（BA）** 等优化方法。学习这些概念后，会更容易理解 **MegaSaM** 是如何在 **动态场景** 下优化相机位姿的。**MegaSaM** 中的 **全局 BA 优化** 是关键创新之一，特别是在相机参数较弱约束的情况下如何提高鲁棒性，这部分内容在 **ORB-SLAM3** 中有类似的优化思想。

【ORB-SLAM3（公开课）】https://www.bilibili.com/video/BV1eCHKeWE31?vd_source=da93815b1fb2de89d97d65055ff8961f

【研究生必学！目前B站最好的【三维重建】教程，15小时深入浅出SfM与SLAM核心算法！计算机视觉3D三维重建/摄像机几何/多视图几何】https://www.bilibili.com/video/BV1Fx4y1t7k1?vd_source=da93815b1fb2de89d97d65055ff8961f

## 视觉SLAM原理和ORB-SLAM3

![image-20250322104443968](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322104443968.png)

### 视觉SLAM原理

​      相机有多种（单目，双目，多目）

SLAM：Simulataneous Localization And Mapping

Localization：解决“我在哪里”的问题：利用传感器确定相机在环境的位置。

Mapping：“我周围环境是什么”问题，构建相机所属环境的地图

二者同时建模，实时最小化二者误差。

![image-20250322095121846](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322095121846.png)

![image-20250322095255892](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322095255892.png)

后端优化接受不管什么价位的相机的图像，处理图像的噪声，进行优化。

### ORB-SLAM历史演变

![image-20250322095546501](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322095546501.png)

单目视频的图像的深度未知，slam有尺度和旋转不变性可以处理透视变化

![image-20250322100241038](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322100241038.png)

![image-20250322100449786](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322100449786.png)

一些规则：不加入深度学习和强化学习（最优关键帧筛选方法）的情况下，主要是人工的经验。

ORB-SLAM-VI：针对单目SLAM缺少尺度信息，不管图像多么模糊，IMU高精度快速计算尺度，重力方向，速度，陀螺仪和加速度偏差，定位摄像机。

![image-20250322101933754](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322101933754.png)

与原始ORB-SLAM有3不同

Atlas：提出一个处理无限数量的非链接的子地图的系统（存不同地图的关键帧）。

![image-20250322102510142](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250322102510142.png)

## 安装并配置ORB-slam3环境(我的环境是ubuntu24.04上的LINUX系统)

### 一、配置eigen

#### 安装

sudo apt-get install libeigen3-dev

#### 测试

mkdir build

cd build

cmake ..

make

./eigenMatrix

显示:

![image-20250405093426709](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250405093426709.png)

### 二、配置 Pangolin

#### 首先下载该软件依赖的

sudo apt-get install libglew-dev  
sudo apt-get install cmake  
sudo apt-get install libboost-dev libboost-thread-dev libboost-filesystem-dev

#### 安装pangolin

到slam的实例包中，就是放第三方库的文件夹下。执行语句：

git clone https://github.com/stevenlovegrove/Pangolin.git 

#### 测试

1. cd Pangolin 
2. mkdir build 
3. cd build 
4. cmake .. 
5. make  
6. ./HelloPangolin
7. 显示一个正方体则安装成功
8. ![image-20250405093700645](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250405093700645.png)



### 三、安装Sophus（李代数库)

SLAM的过程就是不断的估计相机的位姿和建立地图。其中，相机位姿也就是变换矩阵T。

变换矩阵T，我们知道T所在的SE(3)空间，对加法计算并不封闭，也就是说任意两个变换矩阵相加后并不是一个变换矩阵，这主要是因为旋转矩阵对加法是不封闭造成的，它是有约束的。李代数就是解决这个问题的。把（大写）SE空间的T映射为一种叫做李代数的东西，映射后的李代数se。它是由向量组成的，向量是对加法封闭的。这样就可以通过对李代数求导来间接的对变换矩阵求导了。在SLAM里最常说的有两个，一个是特殊正交群SO(3)，也就是旋转矩阵群，还有特殊欧氏群SE(3)，也就是变换矩阵群，3代表是三维的。

李群的定义是指连续光滑的群，比如我们前面说的旋转矩阵群SO(3)，想象拿个杯子就可以在空间中以某个支点连续的旋转它，所以SO(3)它就是李群。如果你一般旋转一边移动它，也是连续的或者说光滑的运动，所以变换矩阵群SE(3)也是李群。

李群空间的任意一个旋转矩阵R都可以用李代数空间的一个向量的反对称矩阵指数来近似。只要记得用旋转矩阵表示的话就是李群空间，而用向量的反对称矩阵表示的话就是李代数空间，这两个空间建立了联系。

详细的解释可以参考连接：https://www.sohu.com/a/270402234_100007727



### 四、安装OpenCV

#### 安装第三方依赖库

引用下载方式：https://blog.csdn.net/YOULANSHENGMENG/article/details/123727975

```
1、首先卸载系统自带的opencv
$ sudo apt-get purge libopencv*
$ sudo apt-get purge python-numpy
$ sudo apt autoremove
2、更新安装的包和库
$ sudo apt-get update
$ sudo apt-get dist-upgrade
3、安装开发工具
#确保pkg包含在install里面
$sudo apt-get install build-essential cmake unzip pkg-config
$ sudo apt-get install cmake libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev
$ sudo apt-get install libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff5-dev libdc1394-22-dev         # 处理图像所需的包
$ sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev liblapacke-dev
$ sudo apt-get install libxvidcore-dev libx264-dev         # 处理视频所需的包
$ sudo apt-get install libatlas-base-dev gfortran          # 优化opencv功能
$ sudo apt-get install ffmpeg

#安装opencv指南的依赖项，视频I/O包，相机流和处理视频文件
$ sudo apt-get install build-essential make cmake cmake-curses-gui
g++ libavformat-dev libavutil-dev
libswscale-dev libv4l-dev libeigen3-dev
libglew-dev libgtk2.0-dev
$ sudo apt-get install libdc1394-22-dev libxine2-dev
libgstreamer1.0-dev
libgstreamer-plugins-base1.0-dev
#安装用于opencv的先决条件，因为OpenCV是一个图像处理/计算机视觉库，需要能够加载标准的图像文件格式，如JPEG、PNG、TIFF等，下面的图像I/O包将允许OpenCV处理图像文件:
sudo apt-get install libjpeg8-dev libjpeg-turbo8-dev libtiff5-dev
libjasper-dev libpng12-dev libavcodec-dev*
#OpenCV的highgui模块依赖于GTK库进行GUI操作，优化opencv的各种库：
$ sudo apt-get install libxvidcore-dev libx264-dev libgtk-3-dev
libatlas-base-dev gfortran
$ sudo apt-get install libopenblas-dev liblapack-dev liblapacke-dev
#安装opencv的头文件和库
$ sudo apt-get install python3-dev python3-pip python3-tk
$ sudo pip3 install numpy
$ sudo pip3 install matplotlib
```

#### 下载opencv的安装包

#下载opencv包

$ cd ~

$ wget -O opencv.zip https://github.com/opencv/opencv/archive/3.4.4.zip

$ unzip opencv.zip

$ mv opencv-3.4.4 opencv

#### 编译

cd opencv

mkdir build

cd build

cmake ..

make

### 五、安装和测试g2o和orb-slam3

使用官方文档的方法基本没有问题，所以查看官方文档：https://github.com/UZ-SLAMLab/ORB_SLAM3

### 六、安装过程错误解决

![image-20250323141029789](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250323141029789.png)

这个错误提示表明，编译器无法找到 `MD5_CTX` 及相关函数（`MD5_Init`、`MD5_Update` 和 `MD5_Final`）的定义，通常这些是 OpenSSL 库的一部分。由于你在使用 OpenSSL 3.0，它移除了对这些函数的支持。

gpt：**安装 OpenSSL 1.1**（如果尚未安装）：

如果你之前尝试过安装 OpenSSL 1.1 并且遇到问题，可以使用以下命令来确保你已经安装了 OpenSSL 1.1：

```
bash


复制编辑
sudo apt-get install libssl1.1
```

**修改 `CMakeLists.txt`**：为了解决 OpenSSL 3.0 中移除的 `MD5_CTX` 和相关函数问题，尝试添加以下标志，强制 OpenSSL 3.0 启用 MD5 支持：

在 `CMakeLists.txt` 中添加：

```
cmake


复制编辑
set(OPENSSL_USE_NODEPREV ON)
```

然后重新编译。没有作用

正确解决方法：`CMakeLists.txt` 目前默认使用 **C++11**，而 `sigslot` 需要 **C++14**。需要修改这个 CMake 配置，使其使用 **C++14**，否则会出现 `auto` 相关错误。

CMakeLists.txt添加c++14的检测逻辑：

```
if(COMPILER_SUPPORTS_CXX14)
   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++14")
   add_definitions(-DCOMPILEDWITHC14)
   //用c++14添加c++11，是因为在后面的cmake的过程中，有一个变量的声明的if条件是if使用c++11，缺少了会出现编译错误，添加了这句也不会引起错误。
   add_definitions(-DCOMPILEDWITHC11)
   message(STATUS "Using flag -std=c++14.")
```



## 视觉特征

### ORB图像特征（Oriented Brief高效特征检测和描述方法）

#### fast算法（features from accelerated segment test）

slam要求实时性，使用fast进行比较比每个都比较16像素点快

![image-20250324092337796](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324092337796.png)

![image-20250324092513908](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324092513908.png)

bresenham圆生成算法，由递推公式生成1/8圆，由镜像对称生成其它点

![image-20250324093528372](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324093528372.png)

```python
代码示例（OpenCV）
import cv2
# 读取图像
img = cv2.imread('image.jpg', 0)
# 创建FAST检测器
fast = cv2.FastFeatureDetector_create(threshold=30)
# 检测角点
kp = fast.detect(img, None)
# 绘制角点
img_kp = cv2.drawKeypoints(img, kp, None, color=(0,255,0))
cv2.imshow('FAST Keypoints', img_kp)
cv2.waitKey(0)
```

#### 特征点尺度不变性

![image-20250331101423644](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331101423644.png)

![image-20250331101443747](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331101443747.png)

#### Brief算法(binary robust independent elementary features)

核心是预定义一组像素对的位置，用于在关键点邻域内进行亮度比较。原始BRIEF：随机生成像素对位置。
改进方法（如ORB）：通过统计学习优化像素对的位置，提高区分度。对每个关键点邻域内的像素对进行亮度比较，生成二进制描述子：组合结果：将所有 (n) 个二进制位按顺序组合成一个二进制字符串（如256位），即得到该关键点的BRIEF描述子。

![image-20250324093720956](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324093720956.png)

![image-20250324094205569](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324094205569.png)
    

```python
#代码示意（伪代码）
def compute_brief_descriptor(image, keypoint, predefined_pairs):
#提取关键点邻域
    patch = extract_patch(image, keypoint)
# 2. 高斯模糊
	patch = gaussian_blur(patch)
# 3. 遍历预定义像素对，生成二进制描述子
	descriptor = []
	for (x1, y1, x2, y2) in predefined_pairs:
	    if patch[y1, x1] > patch[y2, x2]:
	        descriptor.append(1)
	    else:
	        descriptor.append(0)
	return np.array(descriptor, dtype=np.uint8)
```

#### orb对Brief的改进（描述子的可复现性）

以旋转一致性为例

![image-20250324095606304](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324095606304.png)

![image-20250324100423044](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324100423044.png)

![image-20250324100117098](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324100117098.png)使用的是强度质心法（强度指亮度、信号强度等)，图像处理使用亮度

实例演示
假设某图像区域有3个像素点，其坐标和强度如下：

坐标 (x, y)强度 (I)

(1, 2)      10


(3, 4)      20


(5, 6       30



计算质心：
$$
[
\bar{x} = \frac{10 \cdot 1 + 20 \cdot 3 + 30 \cdot 5}{10 + 20 + 30} = \frac{10 + 60 + 150}{60} = \frac{220}{60} \approx 3.67
]
[
\bar{y} = \frac{10 \cdot 2 + 20 \cdot 4 + 30 \cdot 6}{60} = \frac{20 + 80 + 180}{60} = \frac{280}{60} \approx 4.67
]
$$
质心坐标为 ((3.67, 4.67))。

### 图像关键帧

![image-20250324103533737](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324103533737.png)

![image-20250324103841981](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324103841981.png)

![image-20250324104111773](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250324104111773.png)

### 视觉惯性里程计

#### IMU作用

IMU：惯性测量单元，主要包括加速度计和陀螺仪

**加速度计**测得的加速度，实际上是：
$$
a⃗measured=a⃗real−g⃗
$$
所以如果物体静止，IMU 加速度测的就是**负的重力加速度**。

1.单目视觉SLAM缺少尺度信息

2.视觉SLAM通常将第一帧作为世界坐标系，无法知道惯性坐标系

3.纯视觉无法得到准确的速度信息

IMU和视觉可以互补

#### IMU预积分

与传统IMU的运动学积分不同，预积分可以将一段时间内的IMU测量数据累计起来，建立预积分测量，因而十分适合以关键帧为基础单元的激光或[视觉SLAM](https://zhida.zhihu.com/search?content_id=174543542&content_type=Article&match_order=1&q=视觉SLAM&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDUxMTE5MDIsInEiOiLop4bop4lTTEFNIiwiemhpZGFfc291cmNlIjoiZW50aXR5IiwiY29udGVudF9pZCI6MTc0NTQzNTQyLCJjb250ZW50X3R5cGUiOiJBcnRpY2xlIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.l3C7fq6GkmEIhkwdzJfFaaTZXnodH6Z2E-CWCnS_ROI&zhida_source=entity)系统。如今预积分已经成为诸多与IMU紧耦合的标准方法

预积分推导的网址https://zhuanlan.zhihu.com/p/388859808

在一个IMU系统里，我们考虑它的五个变量：旋转 R 、平移 p 、角速度 ω 、线速度 v 与加速度a 。

根据[旋转运动学](https://zhida.zhihu.com/search?content_id=174543542&content_type=Article&match_order=1&q=旋转运动学&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDUxMTE5MDIsInEiOiLml4vovazov5DliqjlraYiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoxNzQ1NDM1NDIsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.X2CURyv7SilKO7r5fDs_reux_3HDasRAlsDpJ09QCto&zhida_source=entity)，这些变量的运动学关系可以写成[5]：
$$
R˙=Rω∧,p˙=v,v˙=a
$$
在 t 到 t+Δt 时间内，对上式进行[欧拉积分](https://zhida.zhihu.com/search?content_id=174543542&content_type=Article&match_order=1&q=欧拉积分&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDUxMTE5MDIsInEiOiLmrKfmi4nnp6_liIYiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoxNzQ1NDM1NDIsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.yDFui7bSbzu6wb9BMaPdjCmJX9BO9aL8fpfG7JRuJds&zhida_source=entity)，可得：
$$
R(t+Δt)=R(t)
Exp(ω(t)Δt),v(t+Δt)=v(t)+a(t)Δt,p(t+Δt)=p(t)+v(t)Δt+12a(t)Δt2
$$
其中角速度和加速度可以被IMU测量到，但受到噪声与重力影响。令测量值为 ω~ 和 a~ ，则：
$$
ω~(t)=ω(t)+bg(t)+ηg(t)
..
a~(t)=RT(a(t)−g)+ba(t)+ηa(t)
$$
其中 bg,ba 为陀螺和加速度计零偏， ηa,ηg 为测量的高斯噪声。把该式代入上式，可得到测量值与状态变量的关系：
$$
R(t+Δt)=R(t)Exp((ω~−bg(t)−ηgd(t))Δt),v(t+Δt)=v(t)+gΔt+R(t)(a~−ba(t)−ηad(t))Δt,p(t+Δt)=p(t)+v(t)Δt+12gΔt2+12R(t)(a~−ba(t)−ηad(t))Δt2
$$
其中 ηgd,ηad 是离散化后的[随机游走噪声](https://zhida.zhihu.com/search?content_id=174543542&content_type=Article&match_order=1&q=随机游走噪声&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDUxMTE5MDIsInEiOiLpmo_mnLrmuLjotbDlmarlo7AiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjoxNzQ1NDM1NDIsImNvbnRlbnRfdHlwZSI6IkFydGljbGUiLCJtYXRjaF9vcmRlciI6MSwiemRfdG9rZW4iOm51bGx9.jWn1KKtKQlNTz4ga3jlYlKlaVpdNxwWjoF6LjGRFesQ&zhida_source=entity)[6]:
$$
Cov(ηgd(t))=1ΔtCov(ηg(t))
$$

$$
Cov(ηad(t))=1ΔtCov(ηa(t))
$$

当然，我们完全可以用这种约束来构建图优化，对IMU相关的问题进行求解。但是这组方程刻画的时间太短，或者说，IMU的测量频率太高。我们并不希望优化过程随着IMU数据进行调用，那样太浪费计算资源。于是，预积分方法应运而生，它可以把一段时间的IMU数据累计起来统一处理。我们不妨假设从关键帧 i 和 j 之间的IMU数据被累计起来，这个过程通常可以持续若干秒钟。这种被累计起来的观测被称为**预积分**[7]。当然，如果我们使用不同形式的运动学，得到的预积分形式也是不同的。那么，在 k 至 j 过程中，我们可以把上式中的变量累计起来，得到：
$$
Rj=Ri∏k=ij−1(Exp((ω~k−bg,k−ηgd,k)Δt)
$$

$$
vj=vi+gΔtij+∑k=ij−1Rk(a~−ba,k−ηad,k)Δt
$$

$$
pj=pi+∑k=ij−1vkΔt+12gΔtij2+12∑k=ij−1Rk(a~k−ba,k−ηad,k)Δt2
$$

其中
$$
Δtij=∑k=ij−1Δt
$$
，为累计起来的时间。在已知 i 时刻状态和所有测量时，该式可以用于推断 j 时刻的状态。当然，这只是运动学式的累计形式，并无本质不同。这就是传统意义上的**直接积分**。

![image-20250429094340752](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250429094340752.png)

直接积分的缺点是，如果我们对 i 时刻的状态进行优化（在IMU预积分中，“对i时刻的状态进行优化”指的是在基于图优化的状态估计框架（如视觉惯性里程计SLAM或滤波平滑算法）中，将关键帧i的状态变量（包括姿态、位置 等）作为优化变量进行调整的过程。这种优化通常发生在多传感器融合的背景下，例如当新的视觉观测或闭环检测引入约束时，需要通过非线性优化方法更新i时刻的状态。），那么 i+1,i+2,…,j−1 时刻的状态也会跟着发生改变，这个积分就必须重新计算，这是非常不便的。为此，我们对上式稍加改变，定义相对的运动量为：


$$
ΔRij=.RiTRj=∏k=ij−1Exp((ω~k−bg,k−ηgd,k)Δt)
$$

$$
Δvij=.RiT(vj−vi−gΔtij)=∑k=ij−1ΔRik(a~k−ba,k−ηad,k)Δt
$$

$$
Δpij=.RiT(pj−pi−viΔtij−12gΔtij2)=∑k=ij−1[ΔvikΔt+12ΔRik(a~k−ba,k−ηad,k)Δt2]
$$

这种改变实际上只是计算了某种从 i 到 j 的“差值”。以上只是预积分的定义，具体推导和计算过程看最上面的网站

##### 假设条件

假设：

静止

每步时间间隔
$$
Δt=0.01s
$$


采样3步

加速度计测量值（静止）：
$$
\tilde{a} =  \begin{bmatrix} 0 \\ 0 \\ 9.8 \end{bmatrix} \ \text{m/s}^2
$$
初始旋转矩阵 
$$
R_i = I（单位阵）
$$

##### 预积分公式

旋转变化：
$$
\Delta R_{ij} = \prod_{k=i}^{j-1} \exp\left( (\tilde{\omega}_k - b_{g,k}) \Delta t \right)
$$
速度变化量：
$$
\Delta v_{ij} = \sum_{k=i}^{j-1} \Delta R_{ik} (\tilde{a}_k - b_{a,k}) \Delta t
$$
位移变化量：
$$
\Delta p_{ij} = \sum_{k=i}^{j-1} \left( \Delta v_{ik} \Delta t + \frac{1}{2} \Delta R_{ik} (\tilde{a}_k - b_{a,k}) \Delta t^2 \right)
$$

##### 具体数值代入计算

由于静止且无旋转
$$
，$\Delta R_{ik} = I$。
$$


###### 第一步

速度变化量增量：
$$
\Delta v_{i,i+1} = I \cdot \tilde{a} \cdot \Delta t =  \begin{bmatrix} 0 \\ 0 \\ 0.098 \end{bmatrix} \text{ m/s}
$$
位移变化量增量：
$$
\Delta p_{i,i+1} = \frac{1}{2} I \tilde{a} \Delta t^2 =  \begin{bmatrix} 0 \\ 0 \\ 0.00049 \end{bmatrix} \text{ m}
$$

###### 第二步

新的速度变化量：
$$
\Delta v_{i,i+2} = \Delta v_{i,i+1} + I \tilde{a} \Delta t =  \begin{bmatrix} 0 \\ 0 \\ 0.196 \end{bmatrix} \text{ m/s}
$$
新的位移变化量：
$$
\Delta p_{i,i+2} = \Delta p_{i,i+1} + \Delta v_{i,i+1} \Delta t + \frac{1}{2} I \tilde{a} \Delta t^2
$$
代入数值得：
$$
\Delta p_{i,i+2} =  \begin{bmatrix} 0 \\ 0 \\ 0.00196 \end{bmatrix} \text{ m}
$$

###### 第三步

新的速度变化量：
$$
\Delta v_{i,i+3} = \Delta v_{i,i+2} + I \tilde{a} \Delta t =  \begin{bmatrix} 0 \\ 0 \\ 0.294 \end{bmatrix} \text{ m/s}
$$
新的位移变化量：
$$
\Delta p_{i,i+3} = \Delta p_{i,i+2} + \Delta v_{i,i+2} \Delta t + \frac{1}{2} I \tilde{a} \Delta t^2
$$
代入数值得：
$$
\Delta p_{i,i+3} =  \begin{bmatrix} 0 \\ 0 \\ 0.00441 \end{bmatrix} \text{ m}
$$


最终预积分结果为：

总速度变化量：
$$
\Delta v_{i,i+3} =  \begin{bmatrix} 0 \\ 0 \\ 0.294 \end{bmatrix} \text{ m/s}
$$


总位移变化量：
$$
\Delta p_{i,i+3} =  \begin{bmatrix} 0 \\ 0 \\ 0.00441 \end{bmatrix} \text{ m}
$$
看起来像是**静止不动**，但是为什么预积分结果 Δv,Δp不是零？

首先要区分两个概念：**IMU测量的“加速度”是去除重力后的还是未去除的？**

**预积分是针对于“IMU自身加速度”，还是针对于全局世界坐标下的运动？**

因为IMU内部实际上测的是：
$$
\text{加速度计读数} = R^T (a - g)
$$
即

a：IMU在世界系下真实的加速度

g：重力加速度（指向地心）

R：IMU的朝向

静止时，a=0a = 0a=0，所以 IMU测量的实际上是：
$$
\text{加速度计读数} = R^T (0 - g) = -R^T g
$$
如果 R=I（初始对齐惯性系），那么加速度计测到的就是：

(0,0,9.8)

###### 预积分正确的处理方式是：

**预积分过程中，要在外层进行重力建模！**

一般预积分是这样做的：

> **内部的预积分只基于IMU测得的变化量，先不减重力。**
>  最后在系统中做优化时，加上重力的影响。

也就是说，内部预积分确实算出了"貌似加速"的量，因为IMU测到了重力。

静止情况下，IMU加速度测量：
$$
\tilde{a} = R^T (0 - g)
$$
所以测量是一个固定值，**不是零**。

因此，预积分公式：
$$
\Delta v_{ij} = \sum_k \Delta R_{ik} \tilde{a}_k \Delta t
$$
自然也会产生非零累积。

真正恢复物理世界运动时，需要在后端加上重力补偿，比如：
$$
v_j = v_i + g \Delta t_{ij} + R_i \Delta v_{ij}
$$

$$
p_j = p_i + v_i \Delta t_{ij} + \frac{1}{2}g\Delta t_{ij}^2 + R_i \Delta p_{ij}
$$
这里的 gΔt就把重力恢复了。

IMU得到角速度和加速度，积分得到速度和位置。

![image-20250331102054392](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331102054392.png)

![image-20250331102215366](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331102215366.png)

![image-20250331102540066](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331102540066.png)

![image-20250331103417387](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331103417387.png)

![image-20250331103523320](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331103523320.png)

#### VIO

![image-20250325103342666](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325103342666.png)

![image-20250325104005708](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325104005708.png)

### 相机模型

#### 针孔模型

![image-20250325104515558](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325104515558.png)

![image-20250325141637402](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325141637402.png)

![image-20250325142858376](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325142858376.png)

外参是相机的旋转和平移。

## 多视图几何算法

![image-20250325143615685](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325143615685.png)

### 2D-2D

![image-20250325143923911](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325143923911.png)

![image-20250325144732546](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325144732546.png)

![image-20250325145049224](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325145049224.png)

介绍八点法：https://blog.csdn.net/u010307048/article/details/104893076

### 3D-2D

![image-20250325152913276](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325152913276.png)

![image-20250325152855919](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325152855919.png)

![image-20250325153007511](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325153007511.png)

![image-20250325152844455](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325152844455.png)

### 3D-3D

![image-20250325153556394](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325153556394.png)

![image-20250325154009058](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250325154009058.png)

## Tracking线程代码

### ORB-SLAM3初始化

![image-20250326083138220](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250326083138220.png)

![image-20250326082433829](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250326082433829.png)

![image-20250326083021770](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250326083021770.png)

### 传感器输入模块

![image-20250326085527314](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250326085527314.png)

### Tracking线程子模块（对应流程图）

![image-20250520101437513](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250520101437513.png)

### 生成关键帧

![image-20250331101629556](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331101629556.png)

### Tracking线程代码

主要是通过grabimage实现获取相机位姿

![image-20250328122742804](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250328122742804.png)

#### Tracking.h

```c++
class Tracking
{  

public:
    EIGEN_MAKE_ALIGNED_OPERATOR_NEW
    //初始化前端
    Tracking(System* pSys, ORBVocabulary* pVoc, FrameDrawer* pFrameDrawer, MapDrawer* pMapDrawer, Atlas* pAtlas,
             KeyFrameDatabase* pKFDB, const string &strSettingPath, const int sensor, Settings* settings, const string &_nameSeq=std::string());

    ~Tracking();
    
    // Parse the config file
    bool ParseCamParamFile(cv::FileStorage &fSettings);//解析相机参数部分（如内参矩阵、畸变系数、图像尺寸等）输入OpenCV 的 FileStorage 对象，已打开 YAML 格式的配置文件,成功解析返回 true
    bool ParseORBParamFile(cv::FileStorage &fSettings);//解析 ORB 特征提取器的参数（如提取层数、特征数、尺度因子等）
    bool ParseIMUParamFile(cv::FileStorage &fSettings);//解析 IMU 参数（如陀螺仪和加速度计的噪声协方差、IMU 到相机的外参等）
    
    // Preprocess the input and call Track(). Extract features and performs stereo matching.主要就是通过三个grabimage实现获取相机位姿，主要作用：（1） 将彩色图转为灰度图（2）构造Frame类（3）跟踪:函数track()
    Sophus::SE3f GrabImageStereo(const cv::Mat &imRectLeft,const cv::Mat &imRectRight, const double &timestamp, string filename);
    Sophus::SE3f GrabImageRGBD(const cv::Mat &imRGB,const cv::Mat &imD, const double &timestamp, string filename);
    Sophus::SE3f GrabImageMonocular(const cv::Mat &im, const double &timestamp, string filename);
    //提取IMU的
    void GrabImuData(const IMU::Point &imuMeasurement);
    //局部建图函数初始化
    void SetLocalMapper(LocalMapping* pLocalMapper);
    //闭环检测
    void SetLoopClosing(LoopClosing* pLoopClosing);
    void SetViewer(Viewer* pViewer);
    //主要用于调试。如果开启，每一帧的处理可以暂停等待手动触发，便于开发者分析每一步的运行情况。
    void SetStepByStep(bool bSet);
    bool GetStepByStep();
    
    // Load new settings
    // The focal lenght should be similar or scale prediction will fail when projecting points
    //重新加载相机标定参数（内参、畸变等），从指定路径的配置文件中读取。切换不同焦距镜头、不同相机模型，或相机软硬件发生变动时使用。
    void ChangeCalibration(const string &strSettingPath);
    
    // Use this function if you have deactivated local mapping and you only want to localize the camera.
    //通知系统当前是否处于“仅跟踪模式”。
    void InformOnlyTracking(const bool &flag);
    //更新当前帧的 IMU 状态，包括预积分所需的时间间隔 s、当前 bias，以及参考关键帧。
    void UpdateFrameIMU(const float s, const IMU::Bias &b, KeyFrame* pCurrentKeyFrame);
    KeyFrame* GetLastKeyFrame()
    {
        return mpLastKeyFrame;
    }
    
    void CreateMapInAtlas();
    //std::mutex mMutexTracks;
    
    //--
    void NewDataset();
    int GetNumberDataset();
    int GetMatchesInliers();
    
    //DEBUG
    void SaveSubTrajectory(string strNameFile_frames, string strNameFile_kf, string strFolder="");
    void SaveSubTrajectory(string strNameFile_frames, string strNameFile_kf, Map* pMap);
    
    float GetImageScale();

#ifdef REGISTER_LOOP
    void RequestStop();
    bool isStopped();
    void Release();
    bool stopRequested();
#endif

public:

    //跟踪状态
    enum eTrackingState{
        SYSTEM_NOT_READY=-1,
        NO_IMAGES_YET=0,
        NOT_INITIALIZED=1,
        OK=2,
        RECENTLY_LOST=3,
        LOST=4,
        OK_KLT=5
    };
    
    eTrackingState mState;//当前帧的
    eTrackingState mLastProcessedState;//上一帧的状态
    
    // Input sensor输入传感器类型
    int mSensor;
    
    // Current Frame
    Frame mCurrentFrame;
    Frame mLastFrame;
    //当前帧的灰度图
    cv::Mat mImGray;
    
    // Initialization Variables (Monocular)
    std::vector<int> mvIniLastMatches;
    std::vector<int> mvIniMatches;
    std::vector<cv::Point2f> mvbPrevMatched;
    std::vector<cv::Point3f> mvIniP3D;
    Frame mInitialFrame;
    
    // Lists used to recover the full camera trajectory at the end of the execution.
    // Basically we store the reference keyframe for each frame and its relative transformation
    list<Sophus::SE3f> mlRelativeFramePoses;//保存每一帧相对于其参考关键帧的位姿（用 Sophus::SE3f 表示 3D 刚体变换）
    list<KeyFrame*> mlpReferences;//保存每一帧对应的参考关键帧指针，用于后期重建相机完整轨迹
    list<double> mlFrameTimes;//保存每一帧图像的时间戳
    list<bool> mlbLost;//记录每一帧是否跟踪失败（true 表示丢失）
    
    // frames with estimated pose
    int mTrackedFr;
    bool mbStep;
    
    // True if local mapping is deactivated and we are performing only localization
    bool mbOnlyTracking;
    
    void Reset(bool bLocMap = false);
    void ResetActiveMap(bool bLocMap = false);
    
    float mMeanTrack;//平均跟踪数，衡量当前帧 ORB 特征点的匹配数量，用于评估跟踪质量
    bool mbInitWith3KFs;//是否使用三个关键帧进行初始化（特别在 IMU 模式下需要多个关键帧进行状态估计）
    double t0; // time-stamp of first read frame
    double t0vis; // time-stamp of first inserted keyframe
    double t0IMU; // time-stamp of IMU initialization
    bool mFastInit = false;//是否使用快速初始化模式（实验性功能，通常用于加快初始化）


    vector<MapPoint*> GetLocalMapMPS();
    
    bool mbWriteStats;
//只有定义宏之后才编译进程序，记录系统各部分运行耗时，以便调试和性能分析。
#ifdef REGISTER_TIMES
    void LocalMapStats2File();//把 局部地图跟踪相关的耗时数据保存到文件里。
    void TrackStats2File();//把整张图像的 跟踪总耗时信息 输出到文件。
    void PrintTimeStats();//将所有耗时数据以可读格式打印出来，便于快速调试或观察。

    vector<double> vdRectStereo_ms;//立体图像矫正时间（双目相机才有）
    vector<double> vdResizeImage_ms;//图像缩放（resize）时间
    vector<double> vdORBExtract_ms;//ORB 特征提取耗时
    vector<double> vdStereoMatch_ms;//双目特征匹配时间
    vector<double> vdIMUInteg_ms;//IMU 预积分计算时间（VIO 模式）
    vector<double> vdPosePred_ms;//位姿预测时间（基于上一帧、IMU、恒速模型）
    vector<double> vdLMTrack_ms;//局部地图追踪耗时
    vector<double> vdNewKF_ms;//关键帧创建与插入耗时
    vector<double> vdTrackTotal_ms;//整体一帧处理的总耗时（包含以上所有）

#endif

protected:

    // Main tracking function. It is independent of the input sensor.
    void Track();
    
    // Map initialization for stereo and RGB-D
    void StereoInitialization();
    
    // Map initialization for monocular
    void MonocularInitialization();
    //void CreateNewMapPoints();
    void CreateInitialMapMonocular();
    
    void CheckReplacedInLastFrame();
    bool TrackReferenceKeyFrame();//与参考关键帧进行匹配
    void UpdateLastFrame();
    bool TrackWithMotionModel();//基于恒速模型预测位姿,恒速模型是假设相机当前帧的运动趋势与前一帧保持一致（位置和方向变化都一样），它用于 TrackWithMotionModel() 中对当前位姿进行预测，以帮助更快更准确地进行地图点匹配和位姿优化。假设你走路拍视频，你的移动通常是平稳的，帧与帧之间的运动差异不大，那么就可以用上一帧的“速度”去预测下一帧的位置。
    bool PredictStateIMU();
    
    bool Relocalization();//当丢失跟踪时进行重定位
    
    void UpdateLocalMap();
    void UpdateLocalPoints();
    void UpdateLocalKeyFrames();
    
    bool TrackLocalMap();
    void SearchLocalPoints();
    
    bool NeedNewKeyFrame();//判断是否需要新关键帧
    void CreateNewKeyFrame();//创建并插入新关键帧
    
    // Perform preintegration from last frame
    void PreintegrateIMU();
    
    // Reset IMU biases and compute frame velocity
    void ResetFrameIMU();
    
    bool mbMapUpdated;
    
    // Imu preintegration from last frame指向从上一个关键帧到当前帧之间 IMU 数据的预积分结果，用于优化状态估计。
    IMU::Preintegrated *mpImuPreintegratedFromLastKF;
    
    // Queue of IMU measurements between frames缓存传感器采集的原始 IMU 数据，形式为时间戳 + 角速度 + 加速度。
    std::list<IMU::Point> mlQueueImuData;
    
    // Vector of IMU measurements from previous to current frame (to be filled by PreintegrateIMU)当前帧从上一帧的时间段内使用的 IMU 数据，用于做预积分。
    std::vector<IMU::Point> mvImuFromLastFrame;
    //用于多线程访问 IMU 队列的互斥锁，防止并发冲突（Tracking 线程/IMU线程共享资源时必须加锁）
    std::mutex mMutexImuQueue;
    
    // Imu calibration parameters指向 IMU 的标定参数结构体，包括：陀螺仪与加速度计噪声 IMU 到相机的外参（位姿变换）
    IMU::Calib *mpImuCalib;
    
    // Last Bias Estimation (at keyframe creation)bias 是 IMU 模型中关键的系统误差，需持续估计和修正。
    IMU::Bias mLastBias;
    
    // In case of performing only localization, this flag is true when there are no matches to
    // points in the map. Still tracking will continue if there are enough matches with temporal points.
    // In that case we are doing visual odometry. The system will try to do relocalization to recover
    // "zero-drift" localization to the map.
    bool mbVO;
    
    //Other Thread Pointers其它模块的指针
    LocalMapping* mpLocalMapper;
    LoopClosing* mpLoopClosing;
    
    //ORB特征提取器
    ORBextractor* mpORBextractorLeft, *mpORBextractorRight;
    ORBextractor* mpIniORBextractor;
    
    //BoW
    ORBVocabulary* mpORBVocabulary;
    KeyFrameDatabase* mpKeyFrameDB;
    
    // Initalization (only for monocular)
    bool mbReadyToInitializate;
    bool mbSetInit;
    
    //Local Map局部地图和地图点
    KeyFrame* mpReferenceKF;
    std::vector<KeyFrame*> mvpLocalKeyFrames;
    std::vector<MapPoint*> mvpLocalMapPoints;
    
    // System
    System* mpSystem;
    
    //可视化工具
    Viewer* mpViewer;
    FrameDrawer* mpFrameDrawer;
    MapDrawer* mpMapDrawer;
    bool bStepByStep;
    
    //Atlas
    Atlas* mpAtlas;
    
    //Calibration matrix
    cv::Mat mK;
    Eigen::Matrix3f mK_;
    cv::Mat mDistCoef;
    float mbf;
    float mImageScale;
    
    float mImuFreq;
    double mImuPer;
    bool mInsertKFsLost;
    
    //New KeyFrame rules (according to fps)
    int mMinFrames;
    int mMaxFrames;
    
    int mnFirstImuFrameId;
    int mnFramesToResetIMU;
    
    // Threshold close/far points
    // Points seen as close by the stereo/RGBD sensor are considered reliable
    // and inserted from just one frame. Far points requiere a match in two keyframes.
    float mThDepth;
    
    // For RGB-D inputs only. For some datasets (e.g. TUM) the depthmap values are scaled.
    float mDepthMapFactor;
    
    //Current matches in frame
    int mnMatchesInliers;
    
    //Last Frame, KeyFrame and Relocalisation Info
    KeyFrame* mpLastKeyFrame;
    unsigned int mnLastKeyFrameId;
    unsigned int mnLastRelocFrameId;
    double mTimeStampLost;
    double time_recently_lost;
    
    unsigned int mnFirstFrameId;
    unsigned int mnInitialFrameId;
    unsigned int mnLastInitFrameId;
    
    bool mbCreatedMap;
    
    //Motion Model
    bool mbVelocity{false};
    Sophus::SE3f mVelocity;
    
    //Color order (true RGB, false BGR, ignored if grayscale)
    bool mbRGB;
    
    list<MapPoint*> mlpTemporalPoints;
    
    //int nMapChangeIndex;
    
    int mnNumDataset;
    
    ofstream f_track_stats;
    
    ofstream f_track_times;
    double mTime_PreIntIMU;
    double mTime_PosePred;
    double mTime_LocalMapTrack;
    double mTime_NewKF_Dec;
    //相机模型
    GeometricCamera* mpCamera, *mpCamera2;
    
    int initID, lastID;
    
    Sophus::SE3f mTlr;
    
    void newParameterLoader(Settings* settings);
#ifdef REGISTER_LOOP
    bool Stop();
    bool mbStopped;
    bool mbStopRequested;
    bool mbNotStop;
    std::mutex mMutexStop;
#endif

public:
    cv::Mat mImRight;
};

} //namespace ORB_SLAM

#endif // TRACKING_H
```



#### Tracking.cc

```c++
//构造函数主要作用：读取相机参数、ORB特征点参数、惯导参数
Tracking::Tracking(System *pSys, ORBVocabulary* pVoc, FrameDrawer *pFrameDrawer, MapDrawer *pMapDrawer,
    Atlas *pAtlas, KeyFrameDatabase* pKFDB, const string &strSettingPath, const int sensor, Settings* settings, const string &_nameSeq)
    : mState(NO_IMAGES_YET), mSensor(sensor), mTrackedFr(0), mbStep(false),
    mbOnlyTracking(false), mbMapUpdated(false), mbVO(false), mpORBVocabulary(pVoc), mpKeyFrameDB(pKFDB),
    mbReadyToInitializate(false), mpSystem(pSys), mpViewer(NULL), bStepByStep(false),
    mpFrameDrawer(pFrameDrawer), mpMapDrawer(pMapDrawer), mpAtlas(pAtlas), mnLastRelocFrameId(0), time_recently_lost(5.0),
    mnInitialFrameId(0), mbCreatedMap(false), mnFirstFrameId(0), mpCamera2(nullptr), mpLastKeyFrame(static_cast<KeyFrame*>(NULL))
{
    // Load camera parameters from settings file
    // Step 1 从配置文件中加载相机参数
    if(settings){
        newParameterLoader(settings);
    }
    else{
        cv::FileStorage fSettings(strSettingPath, cv::FileStorage::READ);
         //读取相机参数
          //如果是ROS，DepthMapFactor应该设为1，即深度不进行缩放
        bool b_parse_cam = ParseCamParamFile(fSettings);
        if(!b_parse_cam)
        {
            std::cout << "*Error with the camera parameters in the config file*" << std::endl;
        }
        // Load ORB parameters
        //读取ORB特征提取的相关参数，该函数中还会
        //根据参数构造ORB提取器mpORBextractorLeft(左目)、mpORBextractorRight(右目)、mpIniORBextractor(初始化用)
        bool b_parse_orb = ParseORBParamFile(fSettings);
        if(!b_parse_orb)
        {
            std::cout << "*Error with the ORB parameters in the config file*" << std::endl;
        }
        //读取imu参数，该函数中还会
        //根据参数构建预积分处理器mpImuPreintegratedFromLastKF
        bool b_parse_imu = true;
        if(sensor==System::IMU_MONOCULAR || sensor==System::IMU_STEREO || sensor==System::IMU_RGBD)
        {
            b_parse_imu = ParseIMUParamFile(fSettings);
            if(!b_parse_imu)
            {
                std::cout << "*Error with the IMU parameters in the config file*" << std::endl;
            }
    
            mnFramesToResetIMU = mMaxFrames;
        }
        if(!b_parse_cam || !b_parse_orb || !b_parse_imu)
        {
            std::cerr << "**ERROR in the config file, the format is not correct**" << std::endl;
            try
            {
                throw -1;
            }
            catch(exception &e)
            {
    
            }
        }
    }
    initID = 0; lastID = 0;
    mbInitWith3KFs = false;
    mnNumDataset = 0;
    // 遍历下地图中的相机，然后打印出来了
    vector<GeometricCamera*> vpCams = mpAtlas->GetAllCameras();
    std::cout << "There are " << vpCams.size() << " cameras in the atlas" << std::endl;
    for(GeometricCamera* pCam : vpCams)
    {
        std::cout << "Camera " << pCam->GetId();
        if(pCam->GetType() == GeometricCamera::CAM_PINHOLE)
        {
            std::cout << " is pinhole" << std::endl;
        }
        else if(pCam->GetType() == GeometricCamera::CAM_FISHEYE)
        {
            std::cout << " is fisheye" << std::endl;
        }
        else
        {
            std::cout << " is unknown" << std::endl;
        }
    }
#ifdef REGISTER_TIMES
    vdRectStereo_ms.clear();
    vdResizeImage_ms.clear();
    vdORBExtract_ms.clear();
    vdStereoMatch_ms.clear();
    vdIMUInteg_ms.clear();
    vdPosePred_ms.clear();
    vdLMTrack_ms.clear();
    vdNewKF_ms.clear();
    vdTrackTotal_ms.clear();
#endif
}
//有两种情况会用PredictStateIMU()：
//（a）视觉跟丢时用imu预测位姿。
//（b）imu模式下，恒速模型跟踪时提供位姿初始值。
//注意：
//（1）此函数不会直接设置当前帧的位姿，而是记录当前帧的imu到世界坐标系的平移、旋转和速度。在后面TrackLocalMap( )中对位姿优化后才设置当前帧的位姿Tcw。
//（2）地图更新(回环、融合、局部BA、IMU初始化时地图会调整)时，利用上一关键帧和距离上一关键帧的预积分，计算当前帧imu的位姿，因为此时关键帧经过了优化调整，认为更准。
//（3）地图未更新时，利用前一帧和距离前一帧的预积分，计算当前帧imu的位姿，因为此时关键帧没有做优化调整，而前一帧距离更近，认为更准。
bool Tracking::PredictStateIMU()
{
    if(!mCurrentFrame.mpPrevFrame)
    {
        Verbose::PrintMess("No last frame", Verbose::VERBOSITY_NORMAL);
        return false;
    }

    if(mbMapUpdated && mpLastKeyFrame)
    {
        const Eigen::Vector3f twb1 = mpLastKeyFrame->GetImuPosition();
        const Eigen::Matrix3f Rwb1 = mpLastKeyFrame->GetImuRotation();
        const Eigen::Vector3f Vwb1 = mpLastKeyFrame->GetVelocity();

        const Eigen::Vector3f Gz(0, 0, -IMU::GRAVITY_VALUE);
        const float t12 = mpImuPreintegratedFromLastKF->dT;

        Eigen::Matrix3f Rwb2 = IMU::NormalizeRotation(Rwb1 * mpImuPreintegratedFromLastKF->GetDeltaRotation(mpLastKeyFrame->GetImuBias()));
        Eigen::Vector3f twb2 = twb1 + Vwb1*t12 + 0.5f*t12*t12*Gz+ Rwb1*mpImuPreintegratedFromLastKF->GetDeltaPosition(mpLastKeyFrame->GetImuBias());
        Eigen::Vector3f Vwb2 = Vwb1 + t12*Gz + Rwb1 * mpImuPreintegratedFromLastKF->GetDeltaVelocity(mpLastKeyFrame->GetImuBias());
        mCurrentFrame.SetImuPoseVelocity(Rwb2,twb2,Vwb2);

        mCurrentFrame.mImuBias = mpLastKeyFrame->GetImuBias();
        mCurrentFrame.mPredBias = mCurrentFrame.mImuBias;
        return true;
    }
    else if(!mbMapUpdated)
    {
        const Eigen::Vector3f twb1 = mLastFrame.GetImuPosition();
        const Eigen::Matrix3f Rwb1 = mLastFrame.GetImuRotation();
        const Eigen::Vector3f Vwb1 = mLastFrame.GetVelocity();
        const Eigen::Vector3f Gz(0, 0, -IMU::GRAVITY_VALUE);
        const float t12 = mCurrentFrame.mpImuPreintegratedFrame->dT;

        Eigen::Matrix3f Rwb2 = IMU::NormalizeRotation(Rwb1 * mCurrentFrame.mpImuPreintegratedFrame->GetDeltaRotation(mLastFrame.mImuBias));
        Eigen::Vector3f twb2 = twb1 + Vwb1*t12 + 0.5f*t12*t12*Gz+ Rwb1 * mCurrentFrame.mpImuPreintegratedFrame->GetDeltaPosition(mLastFrame.mImuBias);
        Eigen::Vector3f Vwb2 = Vwb1 + t12*Gz + Rwb1 * mCurrentFrame.mpImuPreintegratedFrame->GetDeltaVelocity(mLastFrame.mImuBias);

        mCurrentFrame.SetImuPoseVelocity(Rwb2,twb2,Vwb2);

        mCurrentFrame.mImuBias = mLastFrame.mImuBias;
        mCurrentFrame.mPredBias = mCurrentFrame.mImuBias;
        return true;
    }
    else
        cout << "not IMU prediction!!" << endl;

    return false;
}
//算法流程：
//（1） 如果上一帧不存在，则不进行预积分；如果没有imu数据，也不进行预积分，直接返回保存时间戳在两帧之间的imu数据至mvImuFromLastFrame
//（2）构造预积分器pImuPreintegratedFromLastFrame(这个是上一帧到当前帧的预积分)对于n个imu数据，要进行n-1次计算得到两帧之间的预积分量。首先利用中值积分（通过取相邻两个IMU测量值的加速度和角速度的平均值进行积分。该方法能有效减少高频噪声的影响，提高状态估计的鲁棒性。），得到每次计算预积分的加速度和角速度。对于头和尾的Imu数据，由于不能严格地和图像时间戳对齐（IMU与相机的采样频率不同），需要进行适当的补偿（线性插值补偿，得到与图像帧对齐的虚拟测量值后再取均值）。
//（3）开始计算预积分 IntegrateNewMeasurement( )，这里需要计算上一帧到当前帧的预积分pImuPreintegratedFromLastFrame，和上一关键帧到当前帧的预积分mpImuPreintegratedFromLastKF(在初始化帧和插入关键帧时会新建一个，地图更新时，PredictStateIMU需要相对于上一关键帧计算位姿)
//（4）所有imu数据计算完成之后，记录两个预积分值，并设置当前帧为已预积分状态
//线性插值的逻辑：
//
void Tracking::PreintegrateIMU()
{

    if(!mCurrentFrame.mpPrevFrame)
    {
        Verbose::PrintMess("non prev frame ", Verbose::VERBOSITY_NORMAL);
        mCurrentFrame.setIntegrated();
        return;
    }
    mvImuFromLastFrame.clear();
    mvImuFromLastFrame.reserve(mlQueueImuData.size());
    if(mlQueueImuData.size() == 0)
    {
        Verbose::PrintMess("Not IMU data in mlQueueImuData!!", Verbose::VERBOSITY_NORMAL);
        mCurrentFrame.setIntegrated();
        return;
    }
    while(true)
    {
        bool bSleep = false;
        {
            unique_lock<mutex> lock(mMutexImuQueue);
            if(!mlQueueImuData.empty())
            {
                IMU::Point* m = &mlQueueImuData.front();
                cout.precision(17);
                if(m->t<mCurrentFrame.mpPrevFrame->mTimeStamp-mImuPer)
                {
                    mlQueueImuData.pop_front();
                }
                else if(m->t<mCurrentFrame.mTimeStamp-mImuPer)
                {
                    mvImuFromLastFrame.push_back(*m);
                    mlQueueImuData.pop_front();
                }
                else
                {
                    mvImuFromLastFrame.push_back(*m);
                    break;
                }
            }
            else
            {
                break;
                bSleep = true;
            }
        }
        if(bSleep)
            usleep(500);
    }
    const int n = mvImuFromLastFrame.size()-1;
    if(n==0){
        cout << "Empty IMU measurements vector!!!\n";
        return;
    }
    IMU::Preintegrated* pImuPreintegratedFromLastFrame = new IMU::Preintegrated(mLastFrame.mImuBias,mCurrentFrame.mImuCalib);
    for(int i=0; i<n; i++)
    {
        float tstep;
        Eigen::Vector3f acc, angVel;
        if((i==0) && (i<(n-1)))
        {
            float tab = mvImuFromLastFrame[i+1].t-mvImuFromLastFrame[i].t;
            float tini = mvImuFromLastFrame[i].t-mCurrentFrame.mpPrevFrame->mTimeStamp;
            acc = (mvImuFromLastFrame[i].a+mvImuFromLastFrame[i+1].a-
                    (mvImuFromLastFrame[i+1].a-mvImuFromLastFrame[i].a)*(tini/tab))*0.5f;
            angVel = (mvImuFromLastFrame[i].w+mvImuFromLastFrame[i+1].w-
                    (mvImuFromLastFrame[i+1].w-mvImuFromLastFrame[i].w)*(tini/tab))*0.5f;
            tstep = mvImuFromLastFrame[i+1].t-mCurrentFrame.mpPrevFrame->mTimeStamp;
        }
        else if(i<(n-1))
        {
            acc = (mvImuFromLastFrame[i].a+mvImuFromLastFrame[i+1].a)*0.5f;
            angVel = (mvImuFromLastFrame[i].w+mvImuFromLastFrame[i+1].w)*0.5f;
            tstep = mvImuFromLastFrame[i+1].t-mvImuFromLastFrame[i].t;
        }
        else if((i>0) && (i==(n-1)))
        {
            float tab = mvImuFromLastFrame[i+1].t-mvImuFromLastFrame[i].t;
            float tend = mvImuFromLastFrame[i+1].t-mCurrentFrame.mTimeStamp;
            acc = (mvImuFromLastFrame[i].a+mvImuFromLastFrame[i+1].a-
                    (mvImuFromLastFrame[i+1].a-mvImuFromLastFrame[i].a)*(tend/tab))*0.5f;
            angVel = (mvImuFromLastFrame[i].w+mvImuFromLastFrame[i+1].w-
                    (mvImuFromLastFrame[i+1].w-mvImuFromLastFrame[i].w)*(tend/tab))*0.5f;
            tstep = mCurrentFrame.mTimeStamp-mvImuFromLastFrame[i].t;
        }
        else if((i==0) && (i==(n-1)))
        {
            acc = mvImuFromLastFrame[i].a;
            angVel = mvImuFromLastFrame[i].w;
            tstep = mCurrentFrame.mTimeStamp-mCurrentFrame.mpPrevFrame->mTimeStamp;
        }

        if (!mpImuPreintegratedFromLastKF)
            cout << "mpImuPreintegratedFromLastKF does not exist" << endl;
        mpImuPreintegratedFromLastKF->IntegrateNewMeasurement(acc,angVel,tstep);
        pImuPreintegratedFromLastFrame->IntegrateNewMeasurement(acc,angVel,tstep);
    }

    mCurrentFrame.mpImuPreintegratedFrame = pImuPreintegratedFromLastFrame;
    mCurrentFrame.mpImuPreintegrated = mpImuPreintegratedFromLastKF;
    mCurrentFrame.mpLastKeyFrame = mpLastKeyFrame;
    mCurrentFrame.setIntegrated();
    //Verbose::PrintMess("Preintegration is finished!! ", Verbose::VERBOSITY_DEBUG);
}

//恒速运动模型跟踪（Motion Model Tracking） 和 参考关键帧跟踪（TrackReferenceKeyFrame） 是两个重要的跟踪策略，Tracking线程中的恒速运动模型跟踪用于快速、连续帧位姿估计，是主跟踪手段；而参考关键帧跟踪是一个“兜底”机制，在运动模型失效时利用局部地图点恢复当前帧姿态，保证系统鲁棒性和连续性,从而维持系统的连续运行。
//恒速运动模型跟踪
bool Tracking::TrackWithMotionModel()
{
    //构建ORB匹配器 ORBmatcher。
    ORBmatcher matcher(0.9,true);

    // Update last frame pose according to its reference keyframe
    // Create "visual odometry" points if in Localization Mode
    //更新上一帧的位姿和地图点(UpdateLastFrame())，这个函数主要是根据上一帧与它的参考关键帧的相对位姿，乘上它参考关键帧的位姿，来更新上一帧的位姿，即认为相对位姿是准的，而参考关键帧的位姿可能在优化时被调整过了。如果是双目或RGBD模式，还会产生临时地图点，增加匹配，跟踪结束后会删除。
    UpdateLastFrame();
    //如果是imu模式，则调用PredictStateIMU( )来提供位姿估计的初始值。
    if (mpAtlas->isImuInitialized() && (mCurrentFrame.mnId>mnLastRelocFrameId+mnFramesToResetIMU))
    {
        // Predict state with IMU if it is initialized and it doesnt need reset
        PredictStateIMU();
        return true;
    }
    else
    {
        //如果是纯视觉，则用上上帧到上一帧的相对位姿mVelocity，作为上一帧到当前帧的相对位姿，提供位姿估计初始值。
        mCurrentFrame.SetPose(mVelocity * mLastFrame.GetPose());
    }
    fill(mCurrentFrame.mvpMapPoints.begin(),mCurrentFrame.mvpMapPoints.end(),static_cast<MapPoint*>(NULL));
    // Project points seen in previous frame
    int th;
    if(mSensor==System::STEREO)
        th=7;
    else
        th=15;
    //通过将上一帧的地图点投影到当前帧，寻找匹配 matcher.SearchByProjection( )
    int nmatches = matcher.SearchByProjection(mCurrentFrame,mLastFrame,th,mSensor==System::MONOCULAR || mSensor==System::IMU_MONOCULAR);

    // If few matches, uses a wider window search
    if(nmatches<20)
    {
        Verbose::PrintMess("Not enough matches, wider window search!!", Verbose::VERBOSITY_NORMAL);
        fill(mCurrentFrame.mvpMapPoints.begin(),mCurrentFrame.mvpMapPoints.end(),static_cast<MapPoint*>(NULL));
        nmatches = matcher.SearchByProjection(mCurrentFrame,mLastFrame,2*th,mSensor==System::MONOCULAR || mSensor==System::IMU_MONOCULAR);
        Verbose::PrintMess("Matches with wider search: " + to_string(nmatches), Verbose::VERBOSITY_NORMAL);
    }
    if(nmatches<20)
    {
        Verbose::PrintMess("Not enough matches!!", Verbose::VERBOSITY_NORMAL);
        if (mSensor == System::IMU_MONOCULAR || mSensor == System::IMU_STEREO || mSensor == System::IMU_RGBD)
            return true;
        else
            return false;
    }
    // Optimize frame pose with all matches
    //根据获得的匹配点进行位姿优化，Optimizer::PoseOptimization(&mCurrentFrame)
    Optimizer::PoseOptimization(&mCurrentFrame);
    // Discard outliers
    int nmatchesMap = 0;
    for(int i =0; i<mCurrentFrame.N; i++)
    {
        if(mCurrentFrame.mvpMapPoints[i])
        {
            if(mCurrentFrame.mvbOutlier[i])
            {
                MapPoint* pMP = mCurrentFrame.mvpMapPoints[i];

                mCurrentFrame.mvpMapPoints[i]=static_cast<MapPoint*>(NULL);
                mCurrentFrame.mvbOutlier[i]=false;
                if(i < mCurrentFrame.Nleft){
                    pMP->mbTrackInView = false;
                }
                else{
                    pMP->mbTrackInViewR = false;
                }
                pMP->mnLastFrameSeen = mCurrentFrame.mnId;
                nmatches--;
            }
            else if(mCurrentFrame.mvpMapPoints[i]->Observations()>0)
                nmatchesMap++;
        }
    }

    if(mbOnlyTracking)
    {
        mbVO = nmatchesMap<10;
        return nmatches>20;
    }

    if (mSensor == System::IMU_MONOCULAR || mSensor == System::IMU_STEREO || mSensor == System::IMU_RGBD)
        return true;
    else
        return nmatchesMap>=10;
}
//有两种情况会用到此函数：
//（a）刚刚进行了重定位，则跟踪参考关键帧。
//（b）恒速模型为空或恒速跟踪失败。
bool Tracking::TrackReferenceKeyFrame()
{
    // Compute Bag of Words vector
    mCurrentFrame.ComputeBoW();//计算词袋向量mBowVec和特征向量mFeatVec（mBowVec: BoW 向量（词袋的稀疏直方图，⽤于回环等任务）；mFeatVec: 正向索引结构，用于快速匹配用（比如 SearchByBoW）。
    // We perform first an ORB matching with the reference keyframe
    ORBmatcher matcher(0.7,true);
    vector<MapPoint*> vpMapPointMatches;
    //通过BoW加速匹配。用到了mFeatVec，两个关键帧中 只有节点相同的特征点才会被比较，相同节点中的特征点采用暴力搜索，并且需要检查方向性，
    //并且最优的要明显好于次优的。
    int nmatches = matcher.SearchByBoW(mpReferenceKF,mCurrentFrame,vpMapPointMatches);
    if(nmatches<15)
    {
        cout << "TRACK_REF_KF: Less than 15 matches!!\n";
        return false;
    }
    mCurrentFrame.mvpMapPoints = vpMapPointMatches;
    mCurrentFrame.SetPose(mLastFrame.mTcw);//用上一帧的位姿作为初始值
    //mCurrentFrame.PrintPointDistribution();
    // cout << " TrackReferenceKeyFrame mLastFrame.mTcw:  " << mLastFrame.mTcw << endl;
    Optimizer::PoseOptimization(&mCurrentFrame);//位姿优化
    // Discard outliers
    int nmatchesMap = 0;
    //删除外点，统计内点
    for(int i =0; i<mCurrentFrame.N; i++)
    {
        //if(i >= mCurrentFrame.Nleft) break;
        if(mCurrentFrame.mvpMapPoints[i])
        {
            if(mCurrentFrame.mvbOutlier[i])
            {
                MapPoint* pMP = mCurrentFrame.mvpMapPoints[i];
                mCurrentFrame.mvpMapPoints[i]=static_cast<MapPoint*>(NULL);
                mCurrentFrame.mvbOutlier[i]=false;
                if(i < mCurrentFrame.Nleft){
                    pMP->mbTrackInView = false;
                }
                else{
                    pMP->mbTrackInViewR = false;
                }
                pMP->mbTrackInView = false;
                pMP->mnLastFrameSeen = mCurrentFrame.mnId;
                nmatches--;
            }
            else if(mCurrentFrame.mvpMapPoints[i]->Observations()>0)
                nmatchesMap++;
        }
    }
    if (mSensor == System::IMU_MONOCULAR || mSensor == System::IMU_STEREO)
        return true;
    else
        return nmatchesMap>=10;
}

```

#### 线性插值逻辑：

$$
核心思想是通过线性插值，构造虚拟的IMU测量值，使得积分区间严格覆盖图像帧时间t_k,t_{k-1}
$$

##### **(1) 截取 IMU 数据**

$$
从 IMU 队列中截取满足 t \in [t_k, t_{k+1}]的数据，存入 `mvImuFromLastFrame`：
$$

若首个 IMU 时间早于 
$$
t_k
$$
，保留并标记为“需要补偿”。若末尾 IMU 时间早于 $t_{k+1}$，保留并标记为“需要补偿”。

##### **(2) 首尾数据补偿**

对首尾 IMU 数据点进行插值，生成与图像帧时间对齐的虚拟测量值。

##### 情况 1：首个 IMU 数据需要补偿

**场景**：第一个 IMU 时间 
$$
t_0 < t_k
$$


1. 取相邻两个 IMU 数据点：
   $$
   m_0(t_0)
   $$
   和 
   $$
   m_1(t_1)
   $$

2. 计算时间差：

$$
t_{\text{ini}} = t_k - t_0 \\ t_{\text{ab}} = t_1 - t_0
$$

**线性插值公式**：
$$
\mathbf{a}_{\text{virtual}} = \mathbf{a}_0 + \frac{t_{\text{ini}}}{t_{\text{ab}}} (\mathbf{a}_1 - \mathbf{a}_0) \\ \boldsymbol{\omega}_{\text{virtual}} = \boldsymbol{\omega}_0 + \frac{t_{\text{ini}}}{t_{\text{ab}}} (\boldsymbol{\omega}_1 - \boldsymbol{\omega}_0)
$$


#####  情况 2：末尾 IMU 数据需要补偿

**场景**：最后一个 IMU 时间 
$$
t_n < t_{k+1}
$$


1. 取末尾两个 IMU 点：
   $$
   m_{n-1}(t_{n-1})
   $$
    和 
   $$
   m_n(t_n)
   $$
   
2. 计算时间差：

$$
t_{\text{end}} = t_{k+1} - t_n \\ t_{\text{ab}} = t_n - t_{n-1}
$$



**线性插值公式**：
$$
\mathbf{a}_{\text{virtual}} = \mathbf{a}_n + \frac{t_{\text{end}}}{t_{\text{ab}}} (\mathbf{a}_n - \mathbf{a}_{n-1}) \\ \boldsymbol{\omega}_{\text{virtual}} = \boldsymbol{\omega}_n + \frac{t_{\text{end}}}{t_{\text{ab}}} (\boldsymbol{\omega}_n - \boldsymbol{\omega}_{n-1})
$$

#### 恒速运动模型

假设相机从上一帧到当前帧的运动速度和方向与前一帧之间保持不变。

在数学上，如果已知从帧 t−2到帧 t−1 的位姿变换是：
$$
T_{t-1} = T_{t-1, t-2} \cdot T_{t-2}
$$

$$
则我们假设从帧 t−1 到帧 t的变换也是T_{t-1, t-2}，于是得到：
$$

$$
T_t = T_{t-1, t-2} \cdot T_{t-1}
$$

其中：
$$
T_{t-1, t-2} 就是 mVelocity，

T_{t-1}就是上一帧位姿 mLastFrame.GetPose()，

T_t 是当前帧的预测位姿。
$$
mVelocity：这是一个 4x4 齐次变换矩阵，表示从世界坐标系变换到上一帧相机坐标系的姿态：
$$
T_{\text{last}} = \begin{bmatrix} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix}
$$
说明：相机在世界坐标中位移为(1, 0, 0)，朝向不变（单位旋转矩阵）。

mLAstFrame.GetPose():

表示相机从上一帧到当前帧的运动：
$$
\text{mVelocity} = \begin{bmatrix} 1 & 0 & 0 & 0.5 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix}
$$
说明：相机沿 X 轴移动 0.5 个单位，姿态未发生旋转。

根据恒速运动模型，有：
$$
T_{\text{current}} = \text{mVelocity} \cdot T_{\text{last}}
$$
进行矩阵乘法：
$$
\begin{bmatrix} 1 & 0 & 0 & 0.5 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix} \cdot \begin{bmatrix} 1 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 1.5 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix}
$$

#### BoW（bag of words）

一开始用于自然语言处理，词袋模型能够把一个句子转化为向量表示，是比较简单直白的一种方法，它不考虑句子中单词的顺序，只考虑词表中单词在这个句子中的出现次数。

**"John likes to watch movies, Mary likes movies too"**

**"John also likes to watch football games"**

对于这两个句子，我们要用词袋模型把它转化为向量表示，这两个句子形成的词表（不去停用词）为：**[‘also’, ‘football’, ‘games’, ‘john’, ‘likes’, ‘mary’, ‘movies’, ‘to’, ‘too’, ‘watch’]**，向量1：[0,0,0,1,2,1,2,1,1,1],向量2：[1,1,1,1,1,0,0,1,0,1]

```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    "John likes to watch movies, Mary likes movies too",
    "John also likes to watch football games",
]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names())
print(X.toarray())

#输出结果：
#['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']
#[[0 0 0 1 2 1 2 1 1 1]
#[1 1 1 1 1 0 0 1 0 1]]
```

传统视觉领域中 BoW模型的最早提出可以追溯到经典论文："Visual Categorization with Bags of Keypoints"

![Screenshot 2025-05-12 123551](E:\游戏截图\Screenshots\Screenshot 2025-05-12 123551.png)

K-means流程：

<img src="D:\python\K_means Pseudocode.png" alt="K_means Pseudocode" style="zoom: 50%;" />

展示了三个实验的结果。第一个实验探讨了聚类数量对分类器准确率的影响，并评估了朴素贝叶斯分类器的性能。然后，探讨了支持向量机 (SVM) 在同一问题上的性能。前两个实验是在内部七类数据集上进行的。最后一个实验描述了[16]中使用的四类数据集的结果。使用的数据集：

**COIL-100**：

包含 100 个物体，每个物体在不同角度（视角变化）下拍摄 72 张图像。特点：旋转变化丰富，适合测试模型对视角变化的鲁棒性。

**其它测试集**（如 Pascal 类型）：

含复杂背景、光照变化、部分遮挡的图像；有些数据集中人为添加遮挡或亮度扰动来测试模型健壮性。

实验发现：BoW模型在一定程度上能对遮挡、光照、视角变化保持鲁棒；分类效果明显优于传统的基于全局特征的方法；不需要图像的空间布局信息即可进行有效分类。

| 对比维度   | 自然语言（Text BoW） |             图像（视觉 BoW）             |
| ---------- | -------------------- | :--------------------------------------: |
| 基础单位   | 单词                 |         图像描述子（SIFT、ORB）          |
| 词汇表来源 | 所有文档统计出的词典 | 所有图像特征聚类生成的视觉词典（词汇树） |
| 表达方式   | 词频直方图（文本）   |         单词频率向量（视觉单词）         |
| 向量维度   | 词汇数（数千到万）   |       视觉单词数（通常为 10⁴~10⁶）       |
| 忽略顺序   | 是（无序词袋）       |        是（图像中不考虑空间位置）        |

orb-slam中的离线词典就是那个比较大的文件`ORBvoc.txt`，它是DBoW2作者使用orb特征，使用大量图片训练的结果.对每一幅训练图像，提取特征点，将所有这些特征点，通过对描述子（descriptors）聚类的方法，如，k-means++，将其分成若干类（每一类即表示一个单词），即实现聚类的过程，但是由于我们使用的描述子是256维的，那么这个类别（单词）数量不能太小（不然造成误匹配），至少是成千上万，那么这就带来一个查找效率问题，O(N)肯定是不行了，所以视觉BoW一般使用树的结构进行存储,以空间换时间，时间效率将达到log(N)级别.

每个词典树节点表示一组描述子，叶子节点最终代表视觉单词，非叶子节点作为聚类中心帮助快速定位。多个图像描述子如果落在同一个叶子节点上，意味着它们“语义相似”

![image-20250513102255856](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250513102255856.png)

查询过程：

![image-20250513103928221](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250513103928221.png)

对于两个等长的二进制串，它们之间的 **汉明距离 = 不同位的个数**。

A = 10110010  
B = 10011011

从上往下比较：

```
位数：1 0 1 1 0 0 1 0  
     ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓
     1 0 0 1 1 0 1 1
差异位：0 0 1 0 1 0 0 1  => 共 3 位不同
=> 汉明距离 = 3
```

```
int DescriptorDistance(const cv::Mat &a, const cv::Mat &b) {
    const int *pa = a.ptr<int32_t>();
    const int *pb = b.ptr<int32_t>();
	int dist = 0;
	for(int i = 0; i < 8; i++, pa++, pb++) {
	    unsigned int v = *pa ^ *pb; // XOR 异或操作
	    dist += __builtin_popcount(v); // 统计不同位数（汉明距离）
	}
	return dist;
}
void Frame::ComputeBoW()
{
    if(mBowVec.empty())
    {
        vector<cv::Mat> vCurrentDesc = Converter::toDescriptorVector(mDescriptors);
        mpORBvocabulary->transform(vCurrentDesc,mBowVec,mFeatVec,4);
    }
}
int ORBmatcher::SearchByBoW(KeyFrame *pKF1, KeyFrame *pKF2, vector<MapPoint *> &vpMatches12)
{
    const vector<cv::KeyPoint> &vKeysUn1 = pKF1->mvKeysUn;
    const DBoW2::FeatureVector &vFeatVec1 = pKF1->mFeatVec;
    const vector<MapPoint*> vpMapPoints1 = pKF1->GetMapPointMatches();
    const cv::Mat &Descriptors1 = pKF1->mDescriptors;
    
    const vector<cv::KeyPoint> &vKeysUn2 = pKF2->mvKeysUn;
    const DBoW2::FeatureVector &vFeatVec2 = pKF2->mFeatVec;
    const vector<MapPoint*> vpMapPoints2 = pKF2->GetMapPointMatches();
    const cv::Mat &Descriptors2 = pKF2->mDescriptors;
   vpMatches12 = vector<MapPoint*>(vpMapPoints1.size(),static_cast<MapPoint*>(NULL));
    vector<bool> vbMatched2(vpMapPoints2.size(),false);
   vector<int> rotHist[HISTO_LENGTH];
    for(int i=0;i<HISTO_LENGTH;i++)
        rotHist[i].reserve(500);
   const float factor = 1.0f/HISTO_LENGTH;
   int nmatches = 0;
   DBoW2::FeatureVector::const_iterator f1it = vFeatVec1.begin();
    DBoW2::FeatureVector::const_iterator f2it = vFeatVec2.begin();
    DBoW2::FeatureVector::const_iterator f1end = vFeatVec1.end();
    DBoW2::FeatureVector::const_iterator f2end = vFeatVec2.end();
   while(f1it != f1end && f2it != f2end)
    {
        //关键部分，进行Bow特征向量的匹配
        if(f1it->first == f2it->first)
        {
            for(size_t i1=0, iend1=f1it->second.size(); i1<iend1; i1++)
            {
                const size_t idx1 = f1it->second[i1];
                if(pKF1 -> NLeft != -1 && idx1 >= pKF1 -> mvKeysUn.size()){
                    continue;
                }
               MapPoint* pMP1 = vpMapPoints1[idx1];
                if(!pMP1)
                    continue;
                if(pMP1->isBad())
                    continue;
               const cv::Mat &d1 = Descriptors1.row(idx1);
               int bestDist1=256;
                int bestIdx2 =-1 ;
                int bestDist2=256;
                for(size_t i2=0, iend2=f2it->second.size(); i2<iend2; i2++)
                {
                    const size_t idx2 = f2it->second[i2];
                    if(pKF2 -> NLeft != -1 && idx2 >= pKF2 -> mvKeysUn.size()){
                        continue;
                    }
                    MapPoint* pMP2 = vpMapPoints2[idx2];
                    if(vbMatched2[idx2] || !pMP2)
                        continue;
                    if(pMP2->isBad())
                        continue;
                    const cv::Mat &d2 = Descriptors2.row(idx2);
                    int dist = DescriptorDistance(d1,d2);//计算汉明距离
                    if(dist<bestDist1)
                    {
                        bestDist2=bestDist1;
                        bestDist1=dist;
                        bestIdx2=idx2;
                    }
                    else if(dist<bestDist2)
                    {
                        bestDist2=dist;
                    }
                }
               if(bestDist1<TH_LOW)
                {
                    if(static_cast<float>(bestDist1)<mfNNratio*static_cast<float>(bestDist2))
                    {
                        vpMatches12[idx1]=vpMapPoints2[bestIdx2];
                        vbMatched2[bestIdx2]=true;
                       if(mbCheckOrientation)
                        {
                            float rot = vKeysUn1[idx1].angle-vKeysUn2[bestIdx2].angle;
                            if(rot<0.0)
                                rot+=360.0f;
                            int bin = round(rot*factor);
                            if(bin==HISTO_LENGTH)
                                bin=0;
                            assert(bin>=0 && bin<HISTO_LENGTH);
                            rotHist[bin].push_back(idx1);
                        }
                        nmatches++;
                    }
                }
            }
           f1it++;
            f2it++;
        }
        else if(f1it->first < f2it->first)
        {
            f1it = vFeatVec1.lower_bound(f2it->first);
        }
        else
        {
            f2it = vFeatVec2.lower_bound(f1it->first);
        }
    }
   if(mbCheckOrientation)
    {
        int ind1=-1;
        int ind2=-1;
        int ind3=-1;
       ComputeThreeMaxima(rotHist,HISTO_LENGTH,ind1,ind2,ind3);
       for(int i=0; i<HISTO_LENGTH; i++)
        {
            if(i==ind1 || i==ind2 || i==ind3)
                continue;
            for(size_t j=0, jend=rotHist[i].size(); j<jend; j++)
            {
                vpMatches12[rotHist[i][j]]=static_cast<MapPoint*>(NULL);
                nmatches--;
            }
        }
    }
   return nmatches;
}
```

##### mBowVec：

稀疏向量，记录每个视觉单词在图像中出现的次数（可归一化）。常用于图像间余弦相似度、回环检测

定义为：

```
DBoW2::BowVector mBowVec;
```

也就是

```
std::map<WordId, WordValue>
```

设图像中有 3 个视觉单词被激活：

```c++
mBowVec = {
    { 1023, 0.32 },
    { 3741, 0.44 },
    { 8765, 0.24 }
}
```

这表示图像可以用 3 个单词表示：视觉词汇 ID 为 1023、3741 和 8765，分别出现了归一化频率为 0.32、0.44 和 0.24。

##### mFeatVec:

正向索引。记录每个视觉“中间节点”下有哪些图像特征点。常用于加速特征点匹配（特征点对应描述子，实质是ORB 描述子的汉明距离比较，小于阈值匹配成功。）

特征向量则是用于加速图像的匹配。理解特征向量的作用之前，需要先了解一些思路。我们可以把一幅影像中的多个特征点按照描述子分成很多簇，每个簇都有ID，也就是NodeID。这个分割描述子的标准由字典提供，所以在各个影像之间是统一的。这样，比如我们有影像1，提取了100个特征点。按照描述子可以将其分为：节点1包含20个特征点，节点2包含50个特征点，节点3包含10个特征点，节点4包含20个特征点。同理，我们有影像2，提取了200个特征点。按照描述子分为：节点1包含50个特征点，节点2包含70个特征点，节点3包含80个特征点，节点4包含0个特征点。这样，当我们要匹配这两个影像时，从节点1开始，获取影像1中属于节点1的特征点(这里是20个)，再获得影像2中属于节点1的特征点(这里是50个)。我们只要做20对50个特征点的匹配就可以了。做完以后，同理再看节点2，以此类推。这样做的好处就是，可以有效减少匹配计算量，提升匹配效率。举个简单的例子，如果我们采用暴力匹配。对于100对200个特征点的匹配，我们要计算100×200=20000次。而如果对特征点提前分簇，我们要做的计算次数就是20×50+50×70+10×80+20×0=5300。可以看到，减少了几乎四分之三。所以这个策略还是很有效果的。`FeatureVector`类型本质上是一个`std::map`，映射的是`uint`类型的`NodeId`和`vector<unsigned int>`。换句话说，对于一张影像中的特征点而言，它保存的是节点ID和特征点在影像中对应的索引。举个例子，比如我们提取了10个特征点，有4个节点。那么`mFeatVec`包含的内容就是这个样子：

0:{5,2,9}

 1:{3,1}

 2:{4,8}

 3:{6,7}

当我们需要的时候，就可以根据索引直接获取到对应特征点。比如要获取到`Node0`节点下的特征点，那么直接拿着5、2、9这三个索引去`mDescriptors`里找就能获得对应的描述子了。

<img src="C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250516105213028.png" alt="image-20250516105213028" style="zoom: 67%;" /><img src="C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250516105234722.png" alt="image-20250516105234722" style="zoom:67%;" />

<img src="C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250516105303956.png" alt="image-20250516105303956" style="zoom: 80%;" />

SIFT（Scale invariant feature Transform）算法是由David Lowe提出的尺度不变特征转换算法，其目标是解决低层次特征提取及其图像匹配中的实际问题。该算法是一种基于尺度空间，对图像缩放变换保持不变性的图像局部特征描述子。其主要分为三部分进行图像的特征点提取和描述。SURF（Speeded Up Robust Features）是对SIFT算法的加强版本，同时能够加速提取更加鲁棒的特征，是SIFT算子的速度的三倍以上，并且提取出的特征点更有代表性。同时也对描述子的生成以及特征点的匹配进行了优化。

“Bags of Binary Words for Fast Place Recognition in Image Sequences”

![image-20250513153637457](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250513153637457.png)

| 轴       | 含义                | 详细解释                                                     |
| -------- | ------------------- | ------------------------------------------------------------ |
| **横轴** | Recall（召回率）    | 表示系统找出了多少比例的真实匹配（Ground Truth loop closures）。比如总共 100 个真实回环，系统找到了 80 个，那么召回率 = 0.8 |
| **纵轴** | Precision（精确率） | 表示系统找到的候选匹配中有多少是真正的匹配。例如找到了 100 个候选，其中 90 个是真的回环，则精确率 = 0.9 |

Recall = 真阳性 /（真阳性 + 假阴性）

Precision = 真阳性 /（真阳性 + 假阳性）

精确率衡量“你找出的中有多少是对的”；召回率衡量“你应该找出的你找到了多少”。

这张 PR 曲线展示的是在不同相似度阈值 α 下，系统的识别能力。

改变 α 会改变系统“认定是否是相似图像”的严格程度；α 越低 → 容易接受匹配 → Recall ↑，Precision ↓；α 越高 → 严格判断 → Precision ↑，Recall ↓；**曲线越靠近右上角，性能越好**，像一个人回忆照片，如果太宽容（低阈值），就可能认错 → recall 高但 precision 低；太苛刻（高阈值），就只认很确定的图像 → precision 高但 recall 低。

结果表明，对于平面内相机运动的回环检测问题，带有 BRIEF 描述符的 FAST 特征几乎与 SURF 特征一样可靠。其优势在于，它们不仅获取速度更快（每幅图像 13 毫秒，而非 100-400 毫秒），而且占用内存更少（存储一百万字词汇表只需 32 MB，而非 256 MB），并且比较速度更快，从而加快了分层词汇表的使用。

#### Relocalization

当上一帧在跟踪运动模型环节TrackWithMotionModel()或者跟踪参考关键帧环节TrackReferenceKeyFrame跟踪失败时，会对当前帧的跟踪状态有一个判断。

条件1：当前地图中关键帧数目较多（大于10）
条件2（隐藏条件）：当前帧距离上次重定位帧超过1s（说明还比较争气，值的救）
条件3：不是IMU模式
系统会尝试**通过已知的关键帧数据库重新找回相机姿态**，这就是 **重定位（Relocalization）**。同时满足条件1，2 || 1,3 则将状态标记为RECENTLY_LOST;如果是纯视觉模式，则在下一帧会进入重定位函数；如果是VI模式，则在下一帧会通过预积分更新位姿。

重定位的过程大致是：

1. 根据当前帧的 Bag-of-Words（BoW）向量，在关键帧数据库中找出**相似的候选关键帧（候选姿态）**；
2. 与每个候选关键帧做匹配；
3. 用匹配点运行 **PnP + RANSAC + BA** 优化，估计出当前相机的姿态；
4. 如果优化后得到足够多的内点（inliers），认为重定位成功。

```c++
bool Tracking::Relocalization()
{
    Verbose::PrintMess("Starting relocalization", Verbose::VERBOSITY_NORMAL);
    mCurrentFrame.ComputeBoW();//计算当前帧的BoW
	vector<KeyFrame*> vpCandidateKFs = mpKeyFrameDB->DetectRelocalizationCandidates(&mCurrentFrame, mpAtlas->GetCurrentMap());
	//用当前帧的词袋表达（BoW），在当前地图的关键帧数据库中查找最相似的一批关键帧，用于尝试进行姿态重定位。
	if(vpCandidateKFs.empty()) {
	    Verbose::PrintMess("There are not candidates", Verbose::VERBOSITY_NORMAL);
	    return false;
	}
	const int nKFs = vpCandidateKFs.size();
	
	// We perform first an ORB matching with each candidate
	// If enough matches are found we setup a PnP solver
	ORBmatcher matcher(0.75,true);
	vector<MLPnPsolver*> vpMLPnPsolvers;
	vpMLPnPsolvers.resize(nKFs);
	
	vector<vector<MapPoint*> > vvpMapPointMatches;
	vvpMapPointMatches.resize(nKFs);
	
	vector<bool> vbDiscarded;
	vbDiscarded.resize(nKFs);
	
	int nCandidates=0;
	//将计算得到的匹配关系初始化MLPNP求解器pSolver，并重新设置RanSAC迭代求解器的相关参数
	for(int i=0; i<nKFs; i++)
    {
        KeyFrame* pKF = vpCandidateKFs[i];
        // 如果当前帧已经被判定为无效帧，跳过（什么时候会被判定为无效帧？？？？）
        if(pKF->isBad())
            vbDiscarded[i] = true;
        else
        {
            // 当前帧和候选关键帧用BoW进行快速匹配，匹配结果记录在vvpMapPointMatches，nmatches表示匹配的数目
            // vvpMapPointMatches是一个2维vector数组，每一个元素为该关键帧中和当前帧成功匹配的所有特征点id
            int nmatches = matcher.SearchByBoW(pKF,mCurrentFrame,vvpMapPointMatches[i]);
            // 如果匹配数目小于15,认为该帧无效，跳过
            if(nmatches<15)
            {
                vbDiscarded[i] = true;
                continue;
            }
            else
            {
                // 如果匹配数目够用，用匹配结果初始化MLPnPsolver
                // ? 为什么用MLPnP?
                // 参考论文《MLPNP-A REAL-TIME MAXIMUM LIKELIHOOD SOLUTION TO THE PERSPECTIVE-N-POINT PROBLEM》
                MLPnPsolver* pSolver = new MLPnPsolver(mCurrentFrame,vvpMapPointMatches[i]);
                // 初始化Ransac迭代器参数，构造函数调用了一遍，这里重新设置参数
                // 0.99,      // 模型最大概率值，默认0.9
                // 10,        // 内点的最小阈值，默认8
                // 300,       // 最大迭代次数，默认300
                // 6,         // 最小集，每次采样六个点，即最小集应该设置为6，论文里面写着I > 5
                // 0.5,       // 理论最少内点个数，这里是按照总数的比例计算，所以epsilon是比例，默认是0.4
                // 5.991;     // 卡方检验阈值 //This solver needs at least 6 points
                pSolver->SetRansacParameters(0.99,10,300,6,0.5,5.991);  //This solver needs at least 6 points
                // 对于每一对较好的匹配，都需要使用匹配结果初始化MLPNP求解器
                vpMLPnPsolvers[i] = pSolver;
                nCandidates++;
            }
        }
    }
	
	// Alternatively perform some iterations of P4P RANSAC
	// Until we found a camera pose supported by enough inliers
//    bMatch = false
//for (所有候选关键帧 i)
//{
    // 从候选关键帧生成的 PnP 求解器中取一个
//    pSolver = vpMLPnPsolvers[i]

    // 迭代5次 RANSAC，尝试计算当前帧姿态
 //   bool bTcw = pSolver->iterate(5, bNoMore, vbInliers, nInliers, eigTcw)
//
 //   if (bNoMore)  // 达到最大迭代次数就丢弃这个候选帧
 //       vbDiscarded[i] = true

 //   if (bTcw) {   // 找到了一个可能的相机姿态解
 //       优化当前帧姿态(PnP + BA)
 //       如果内点数太少( <50 ):
 //           进一步用投影搜索补点，再优化一次
//        如果最终优化内点数>=50:
//            bMatch = true (重定位成功)
//            break
//    }
//}
	bool bMatch = false;
	ORBmatcher matcher2(0.9,true);
	while(nCandidates>0 && !bMatch)
	{
	    for(int i=0; i<nKFs; i++)
	    {
	        if(vbDiscarded[i])
	            continue;
	
	        // Perform 5 Ransac Iterations
	        vector<bool> vbInliers;
	        int nInliers;
	        bool bNoMore;
	
	        MLPnPsolver* pSolver = vpMLPnPsolvers[i];
	        Eigen::Matrix4f eigTcw;
	        bool bTcw = pSolver->iterate(5,bNoMore,vbInliers,nInliers, eigTcw);
	
	        // If Ransac reachs max. iterations discard keyframe 
	        if(bNoMore)
	        {
	            vbDiscarded[i]=true;
	            nCandidates--;
	        }
	
	        // If a Camera Pose is computed, optimize
	        if(bTcw)
	        {
	            Sophus::SE3f Tcw(eigTcw);
	            mCurrentFrame.SetPose(Tcw);
	            // Tcw.copyTo(mCurrentFrame.mTcw);
	            set<MapPoint*> sFound;
	            const int np = vbInliers.size();
	            for(int j=0; j<np; j++)
	            {
	                if(vbInliers[j])
	                {
	                    mCurrentFrame.mvpMapPoints[j]=vvpMapPointMatches[i][j];
	                    sFound.insert(vvpMapPointMatches[i][j]);
	                }
	                else
	                    mCurrentFrame.mvpMapPoints[j]=NULL;
	            }
	
	            int nGood = Optimizer::PoseOptimization(&mCurrentFrame);
	            if(nGood<10)
	                continue;
	
	            for(int io =0; io<mCurrentFrame.N; io++)
	                if(mCurrentFrame.mvbOutlier[io])
	                    mCurrentFrame.mvpMapPoints[io]=static_cast<MapPoint*>(NULL);
	
	            // If few inliers, search by projection in a coarse window and optimize again
	            if(nGood<50)
	            {
	                int nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,10,100);
	
	                if(nadditional+nGood>=50)
	                {
	                    nGood = Optimizer::PoseOptimization(&mCurrentFrame);
	
	                    // If many inliers but still not enough, search by projection again in a narrower window
	                    // the camera has been already optimized with many points
	                    if(nGood>30 && nGood<50)
	                    {
	                        sFound.clear();
	                        for(int ip =0; ip<mCurrentFrame.N; ip++)
	                            if(mCurrentFrame.mvpMapPoints[ip])
	                                sFound.insert(mCurrentFrame.mvpMapPoints[ip]);
	                        nadditional =matcher2.SearchByProjection(mCurrentFrame,vpCandidateKFs[i],sFound,3,64);
	
	                        // Final optimization
	                        if(nGood+nadditional>=50)
	                        {
	                            nGood = Optimizer::PoseOptimization(&mCurrentFrame);
	
	                            for(int io =0; io<mCurrentFrame.N; io++)
	                                if(mCurrentFrame.mvbOutlier[io])
	                                    mCurrentFrame.mvpMapPoints[io]=NULL;
	                        }
	                    }
	                }
	            }
	                        // If the pose is supported by enough inliers stop ransacs and continue
	            if(nGood>=50)
	            {
	                bMatch = true;
	                break;
	            }
	        }
	    }
	}
	
	if(!bMatch)
	{
	    return false;
	}
	else
	{
	    mnLastRelocFrameId = mCurrentFrame.mnId;
	    cout << "Relocalized!!" << endl;
	    return true;
	}
}
```

##### DetectRelocalizationCandidates()

遍历当前帧词袋向量中的每一个单词，对于每一个单词，会在关键帧数据库中寻找包含该单词的关键帧，并记录该关键帧与当前帧的公共单词数量pKFi->mnRelocWords。如果找不到候选关键帧，本次重定位就宣告失败了。

```c++
// 步骤1：找出和当前帧具有公共单词的所有关键帧
vector<KeyFrame*> KeyFrameDatabase::DetectRelocalizationCandidates(Frame  , Map* pMap)
{
    list<KeyFrame*> lKFsSharingWords;
{
    unique_lock<mutex> lock(mMutex);
    // 遍历当前帧所有的单词
    for(DBoW2::BowVector::const_iterator vit=F->mBowVec.begin(), vend=F->mBowVec.end(); vit != vend; vit++)
    {
        // lKFs表示包含了第vit->first个单词的所有关键帧
        // 在mvInvertedFile中，每一个单词都对应一个list<KeyFrame*>
        list<KeyFrame*> &lKFs = mvInvertedFile[vit->first];
        // 遍历这些关键帧
        for(list<KeyFrame*>::iterator lit=lKFs.begin(), lend= lKFs.end(); lit!=lend; lit++)
        {
            KeyFrame* pKFi=*lit;
            // 如果pKFi还没有标记为重定位的候选帧，将该关键帧重定位相关的属性初始化一下
            // 因为两帧之间可能有多个公共单词，所以在遍历之前的单词时就将该关键帧进行了标记
            if(pKFi->mnRelocQuery!=F->mnId)
            {
                pKFi->mnRelocWords=0;// 共有的单词数
                pKFi->mnRelocQuery=F->mnId;// 具有相同单词的F的ID
                lKFsSharingWords.push_back(pKFi);// 放入候选
            }
            // 如果已经被标记过了，直接将公共单词数自增1
            pKFi->mnRelocWords++;
        }
    }
}
	if(lKFsSharingWords.empty())
        return vector<KeyFrame*>();
//步骤2：只和具有共同单词较多的关键帧进行相似度计算
//为了进一步保证重定位的鲁棒性，在候选关键帧比较多的的情况下会择优进行筛选，挑选和当前帧公共单词较多的关键帧进行后面相似度计算等相关操作
    // Only compare against those keyframes that share enough words
    int maxCommonWords=0;
    for(list<KeyFrame*>::iterator lit=lKFsSharingWords.begin(), lend= lKFsSharingWords.end(); lit!=lend; lit++)
    {
        if((*lit)->mnRelocWords>maxCommonWords)
            maxCommonWords=(*lit)->mnRelocWords;
    }
    int minCommonWords = maxCommonWords*0.8f;

    list<pair<float,KeyFrame*> > lScoreAndMatch;

    int nscores=0;
    // Compute similarity score.
    // 遍历所有闭环候选帧，挑选出共有单词数大于阈值minCommonWords和相似性得分
  for(list<KeyFrame*>::iterator lit=lKFsSharingWords.begin(), lend= lKFsSharingWords.end(); lit!=lend; lit++)
    {
        KeyFrame* pKFi = *lit;
        if(pKFi->mnRelocWords > minCommonWords)
        {
            //nscores++;
            float si = mpVoc->score(F->mBowVec,pKFi->mBowVec);// 计算相似性得分
            pKFi->mRelocScore=si;// 记录相似性得分
            lScoreAndMatch.push_back(make_pair(si,pKFi));// 先不进行筛选，全都放到容器中
        }
    }

    if(lScoreAndMatch.empty())
        return vector<KeyFrame*>();

    list<pair<float,KeyFrame*> > lAccScoreAndMatch;
    float bestAccScore = 0;
    // Lets now accumulate score by covisibility
    // 单单计算当前帧和某一关键帧的相似性是不够的，这里将与关键帧相连（权值最高，共视程度最高）的前十个关键帧归为一组，计算累计得分
    // 具体而言：lScoreAndMatch中每一个KeyFrame都把与自己共视程度较高的帧归为一组，每一组会计算组得分并记录该组分数最高的KeyFrame，记录于lAccScoreAndMatch
    for(list<pair<float,KeyFrame*> >::iterator it=lScoreAndMatch.begin(), itend=lScoreAndMatch.end(); it!=itend; it++)
    {
        // 得到与该关键帧连接的前N个关键帧，如果连接的关键帧少于N，则返回所有连接的关键帧
        KeyFrame* pKFi = it->second;
        vector<KeyFrame*> vpNeighs = pKFi->GetBestCovisibilityKeyFrames(10);

        float bestScore = it->first;// 该组最高分数,用于代表这一个“组”的代表性关键帧
        float accScore = bestScore;// 该组累计得分,用来度量一个关键帧组的“整体匹配可信度”。
        KeyFrame* pBestKF = pKFi;// 该组最高分数对应的关键帧
        // 遍历该组的10个关键帧
        for(vector<KeyFrame*>::iterator vit=vpNeighs.begin(), vend=vpNeighs.end(); vit!=vend; vit++)
        {
            KeyFrame* pKF2 = *vit;
            if(pKF2->mnRelocQuery!=F->mnId)
                continue;
            accScore+=pKF2->mRelocScore;// 只有pKF2也在闭环候选帧中(重定位候选帧中)，才能贡献分数
            if(pKF2->mRelocScore>bestScore)// 统计得到组里分数最高的KeyFrame
            {
                pBestKF=pKF2;
                bestScore = pKF2->mRelocScore;
            }
        }
        lAccScoreAndMatch.push_back(make_pair(accScore,pBestKF));// 记录所有组中组得分最高的关键帧
        if(accScore>bestAccScore)
            bestAccScore=accScore;// 得到所有组中最高的累计得分
    }

    // Return all those keyframes with a score higher than 0.75*bestScore
    float minScoreToRetain = 0.75f*bestAccScore;
    set<KeyFrame*> spAlreadyAddedKF;
    vector<KeyFrame*> vpRelocCandidates;
    vpRelocCandidates.reserve(lAccScoreAndMatch.size());
    for(list<pair<float,KeyFrame*> >::iterator it=lAccScoreAndMatch.begin(), itend=lAccScoreAndMatch.end(); it!=itend; it++)
    {
        const float &si = it->first;
        if(si>minScoreToRetain)
        {
            KeyFrame* pKFi = it->second;
            if (pKFi->GetMap() != pMap)
                continue;
            if(!spAlreadyAddedKF.count(pKFi))
            {
                vpRelocCandidates.push_back(pKFi);
                spAlreadyAddedKF.insert(pKFi);
            }
        }
    } 	

    return vpRelocCandidates;
}


```

| 角色             | 类比意义                               |
| ---------------- | -------------------------------------- |
| bestScore        | 这个人所在小组里，谁最优秀？选出代表   |
| accScore         | 整个小组的总能力，用于评价这个组强不强 |
| bestAccScore     | 所有小组中最强的组的得分，用作基准     |
| minScoreToRetain | 是一个筛选线，只保留75%得分以上的组    |

既然前面已经筛选了“公共单词足够多”的关键帧，为什么还要考虑“组内可能有更优关键帧”？

涉及词袋（BoW）方法和地图拓扑之间的**信息不对称性**。

前面筛的是 BoW 单词，后面组是基于地图拓扑关系（共视关系）

假设：

当前帧 F 观察到了一个教室；

候选关键帧 A 是前面某次进入该教室记录下来的，BoW 单词有很多共同的；

但 A 的光照较暗，BoW 匹配得分不够高；

A 的共视帧 B 是当时教室里的另一帧，光照正常，BoW 匹配得分更好。

这时你就希望通过 A 把 B 找出来，并且选择得分更高的 B 来闭环！（“公共单词少”≠“匹配得分差”！！！）

公共单词筛选：完全是基于视觉词袋（BoW）空间计算；不考虑关键帧之间是否互相关联；比如某帧因为光照变化或动态物体遮挡，单词数可能低估。

组内共视帧：这些帧可能和当前帧具有更稳定的几何约束，只是 BoW 层面没有表现出来；共视帧彼此之间姿态已知，也许反而是几何上更好匹配的候选帧。

这是 ORB-SLAM 中最精妙的闭环策略之一，它融合了两个世界的信息：

| 信息类型       | 来源     | 特点                     |
| -------------- | -------- | ------------------------ |
| BoW 匹配       | 图像外观 | 快速，但对遮挡/光照敏感  |
| 共视关系组得分 | 地图结构 | 几何稳定，但不能独立定位 |

##### score的计算

该函数通实现的是：**L1-norm 形式的交集相似度计算**：
$$
\text{score}(v_1, v_2) = \sum_{i \in (v_1 \cap v_2)} \min(v_1[i], v_2[i])
$$
即：对于两个 BoW 向量中都包含的单词，取其 TF-IDF 权重（它衡量的是一个词在某篇文章中**出现得多**，但在整个语料库中**不常见**，
 那它很可能是这篇文章的重要关键词）的最小值。所有这类最小值的总和，就是这两个帧的 **相似性得分**。得分范围通常在[0,1]，越接近 1 表示相似度越高。

当前帧 F 的 BoW 向量（`v1`）：

```
v1 = {
    10: 0.2,
    20: 0.4,
    30: 0.1,
    40: 0.3
}
```

候选关键帧的 BoW 向量（`v2`）：

```
v2 = {
    20: 0.3,
    30: 0.2,
    50: 0.4
}
```

我们只考虑两个向量中都包含的单词（`v1 ∩ v2`）：共同单词是：20 和 30；对于每一个共同单词，取其在两个向量中的最小值（`min(v1[i], v2[i])`）

最后得分：0.3+0.1=0.4

##### `KeyFrame::isBad()` 

会在以下几种场景中返回 `true`，即关键帧已被“标记为废弃”：

场景一：该关键帧已被剔除或替代

- 在局部地图维护、闭环、回环修正等过程中，如果一个关键帧被判定为冗余（如其信息已被其他帧覆盖），就会被标记为 bad。
- 典型调用：`KeyFrame::SetBadFlag()`

场景二：地图优化中丢失帧或冗余帧被回收

- 局部 BA 优化后，一些贡献度低、重复率高的关键帧会被清理。
- 比如共视帧太少，地图点很少，信息增益低。

场景三：闭环修正时被替换或融合进别的关键帧

- 回环时会合并一些帧并删除多余帧，避免图优化过于复杂。

#### PnP（Perspective-n-Point）算法

PnP 问题是指：在相机内参已知的情况下，给定n 个三维点（3D）在世界坐标系中的位置，以及它们在图像中的二维投影点（2D），求解相机相对于世界坐标系的位姿（旋转 R 和平移 t）。

你知道一个物体在现实世界中的 3D 坐标，桌角、门把手的具体位置，也知道它们出现在照片中的像素位置。
 现在，你想知道：拍这张照片时，相机在什么位置？朝哪个方向？

这就是 PnP 问题。

PnP问题的几何结构如下图所示，给定3D点的坐标以及对应2D点的坐标以及内参矩阵，求解相机的姿态。

![image-20250521112346638](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250521112346638.png)

##### MLPnP

![image-20250522112134210](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250522112134210.png)

最大似然在 MLPnP 中的作用（MLPNP - A REAL-TIME MAXIMUM LIKELIHOOD SOLUTION TO THE PERSPECTIVE-N-POINT PROBLEM）

 传统 PnP 的问题：很多经典 PnP 方法（如 EPnP）只是几何方法，不考虑观测的不确定性

它们对图像噪声没有特别好的鲁棒性，容易被外点影响

MLPnP 的改进点：

MLPnP 把图像点观测看成是“随机变量”；假设观测误差满足某种概率分布（通常是高斯分布；构建“似然函数”来衡量给定位姿（R,t）下，观测发生的概率；然后使用最大似然原则，寻找使这个概率最大的 R,t

换句话说：
$$
\hat{R}, \hat{t} = \arg\max_{R, t} \prod_{i} P(\mathbf{x}_i' | \mathbf{X}_i, R, t)
$$
或者转成最小化负对数似然（更常见）：
$$
\hat{R}, \hat{t} = \arg\min_{R, t} \sum_i (\mathbf{x}_i' - \pi(K [R | t] \mathbf{X}_i))^T \Sigma_i^{-1} (\mathbf{x}_i' - \pi(K [R | t] \mathbf{X}_i))
$$

$$
其中 
\
\Sigma_i

是每个观测的协方差矩阵，用来加权误差（不确定性越大，权重越小）。
$$


##### 最大似然估计

通俗理解来说，**就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！**

**换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。**

**极大似然估计中采样需满足一个重要的假设，就是所有的采样都是独立同分布的。**

假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？

很多人马上就有答案了：70%。而其后的理论支撑是什么呢？

我们假设罐中白球的比例是p，那么黑球的比例就是1-p。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，**所以每次抽出来的球的颜 色服从同一独立分布。**

这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的,三十次为黑球事件的概率是P(样本结果|Model)。

如果第一次抽象的结果记为x1,第二次抽样的结果记为x2....那么样本结果为(x1,x2.....,x100)。这样，我们可以得到如下表达式：

P(样本结果|Model)

　　= P(x1,x2,…,x100|Model)

　　= P(x1|Mel)P(x2|M)…P(x100|M)

　　= p^70(1-p)^30.

好的，我们已经有了观察样本结果出现的概率表达式了。那么我们要求的模型的参数，也就是求的式中的p。

那么我们怎么来求这个p呢？

不同的p，直接导致P（样本结果|Model）的不同。

好的，我们的p实际上是有无数多种分布的。如下：



![img](https://picx.zhimg.com/v2-19c3773b9b9a6130dc2d8be535add249_1440w.jpg)

那么求出 p^70(1-p)^30为 7.8 * 10^(-31)

p的分布也可以是如下：



![img](https://pic1.zhimg.com/v2-66fe8b6dc2b50ad0d220b4ecc01ad45a_1440w.jpg)

那么也可以求出p^70(1-p)^30为2.95* 10^(-27)

那么问题来了，既然有无数种分布可以选择，极大似然估计应该按照什么原则去选取这个分布呢？

答：采取的方法是让这个样本结果出现的可能性最大，也就是使得p^70(1-p)^30值最大，那么我们就可以看成是p的方程，求导即可！

那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是最大似然估计的核心。

我们想办法让观察样本出现的概率最大，转换为数学问题就是使得：

p^70(1-p)^30最大，这太简单了，未知数只有一个p，我们令其导数为0，即可求出p为70%，与我们一开始认为的70%是一致的。其中蕴含着我们的数学思想在里面。

## Local Mapping线程代码

![image-20250331104032622](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331104032622.png)

![image-20250331104138778](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331104138778.png)

**![image-20250331104510713](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331104510713.png)**

![image-20250331105000835](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331105000835.png)

![image-20250331105214365](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331105214365.png)

![image-20250331105424300](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250331105424300.png)

![image-20250404093823395](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250404093823395.png)

MapPointCulling剔除没用的地图点，

在LocalMapping.cc这个文件中主要做了两件事，一是局部关键帧和地图点的优化，二是IMU的初始化。
根据ORB-SLAM3论文和代码，imu的初始化一共有三个阶段：

纯视觉的MAP估计。系统运行2s，地图中大概有10个关键帧和几百个地图点时，进行一次纯视觉的局部BA，优化关键帧位姿和地图点。

仅imu的MAP估计。估计imu的速度、bias(初始化过程中假设不变)、重力方向旋转矩阵Rwg(将第一帧相机确定的世界坐标系的负z轴旋转到与重力方向重合的方向)、尺度s。通过预积分构建imu残差，利用图优化的方法估计状态量。

视觉imu联合MAP估计。将视觉残差和imu残差联合起来，进行全局优化，优化相机位姿、imu速度、bias、地图点。![屏幕截图 2025-04-21 102613](E:\游戏截图\Screenshots\屏幕截图 2025-04-21 102613.png)

### LocalMapping.h

```c++
//LocalMapping.h
class LocalMapping
{
public:
    EIGEN_MAKE_ALIGNED_OPERATOR_NEW
    LocalMapping(System* pSys, Atlas* pAtlas, const float bMonocular, bool bInertial, const string &_strSeqName=std::string());
    //
    void SetLoopCloser(LoopClosing* pLoopCloser);

void SetTracker(Tracking* pTracker);

// Main function
void Run();

void InsertKeyFrame(KeyFrame* pKF);
void EmptyQueue();

// Thread Synch
void RequestStop();//当需要关闭 SLAM、进行闭环检测（Loop Closing）或全局 BA 时，先调用这个函数发出“停一停”的请求。请求停止 LocalMapping 线程
void RequestReset();
void RequestResetActiveMap(Map* pMap);//请求只重置当前活动地图
bool Stop();
void Release();
bool isStopped();
bool stopRequested();//一般和 RequestStop() 配合使用，主线程调用 RequestStop() 后可以使用这个来轮询判断。
bool AcceptKeyFrames();
void SetAcceptKeyFrames(bool flag);
bool SetNotStop(bool flag);

void InterruptBA();

void RequestFinish();
bool isFinished();

int KeyframesInQueue(){
    unique_lock<std::mutex> lock(mMutexNewKFs);
    return mlNewKeyFrames.size();
}

bool IsInitializing();
double GetCurrKFTime();
KeyFrame* GetCurrKF();

std::mutex mMutexImuInit;

Eigen::MatrixXd mcovInertial;
Eigen::Matrix3d mRwg;
Eigen::Vector3d mbg;
Eigen::Vector3d mba;
double mScale;
double mInitTime;
double mCostTime;

unsigned int mInitSect;
unsigned int mIdxInit;
unsigned int mnKFs;
double mFirstTs;
int mnMatchesInliers;

// For debugging (erase in normal mode)
int mInitFr;
int mIdxIteration;
string strSequence;

bool mbNotBA1;
bool mbNotBA2;
bool mbBadImu;

bool mbWriteStats;

// not consider far points (clouds)
bool mbFarPoints;
float mThFarPoints;

#ifdef REGISTER_TIMES
    vector<double> vdKFInsert_ms;
    vector<double> vdMPCulling_ms;
    vector<double> vdMPCreation_ms;
    vector<double> vdLBA_ms;
    vector<double> vdKFCulling_ms;
    vector<double> vdLMTotal_ms;

vector<double> vdLBASync_ms;
vector<double> vdKFCullingSync_ms;
vector<int> vnLBA_edges;
vector<int> vnLBA_KFopt;
vector<int> vnLBA_KFfixed;
vector<int> vnLBA_MPs;
int nLBA_exec;
int nLBA_abort;

#endif
protected:

bool CheckNewKeyFrames();
void ProcessNewKeyFrame();
void CreateNewMapPoints();

void MapPointCulling();
void SearchInNeighbors();
void KeyFrameCulling();

System *mpSystem;

bool mbMonocular;
bool mbInertial;

void ResetIfRequested();
bool mbResetRequested;
bool mbResetRequestedActiveMap;
Map* mpMapToReset;
std::mutex mMutexReset;

bool CheckFinish();
void SetFinish();
bool mbFinishRequested;
bool mbFinished;
std::mutex mMutexFinish;

Atlas* mpAtlas;

LoopClosing* mpLoopCloser;
Tracking* mpTracker;

std::list<KeyFrame*> mlNewKeyFrames;

KeyFrame* mpCurrentKeyFrame;

std::list<MapPoint*> mlpRecentAddedMapPoints;

std::mutex mMutexNewKFs;

bool mbAbortBA;

bool mbStopped;
bool mbStopRequested;
bool mbNotStop;
std::mutex mMutexStop;

bool mbAcceptKeyFrames;
std::mutex mMutexAccept;

void InitializeIMU(float priorG = 1e2, float priorA = 1e6, bool bFirst = false);
void ScaleRefinement();

bool bInitializing;

Eigen::MatrixXd infoInertial;
int mNumLM;
int mNumKFCulling;

float mTinit;

int countRefinement;

//DEBUG
ofstream f_lm;

};
```

### InitializeIMU

```c++
void LocalMapping::InitializeIMU(float priorG, float priorA, bool bFIBA)//部分代码
{

bInitializing = true;

while(CheckNewKeyFrames())
{
    ProcessNewKeyFrame();
    vpKF.push_back(mpCurrentKeyFrame);
    lpKF.push_back(mpCurrentKeyFrame);
}
const int N = vpKF.size();
IMU::Bias b(0,0,0,0,0,0);
// Compute and KF velocities mRwg estimation
//1.首先检查地图中的关键帧数量和已运行时间，关键帧大于10帧，时间大于1s(非单目)/2s(单目)。计算速度V和重力方向cvRwg的初始值。
//IMU 提供的是加速度（含重力）和角速度数据，但它不知道“哪个方向是地面”。所以我们要通过预积分和速度估计，反推出重力方向，并构造 cvRwg 把世界坐标系的 z 轴旋转到和重力方向一致。这个过程就像给 SLAM 系统“竖正”方向，告诉它“下”在哪儿。后面在 VIO/VINS 模块中还会进一步优化这个旋转方向，但第一次初始化时必须有个初值，这就是 cvRwg 的作用。
if (!mpCurrentKeyFrame->GetMap()->isImuInitialized())//imu还未初始化
    {
        cv::Mat cvRwg;//世界坐标系(第一帧相机)到重力方向的旋转矩阵, 3x3 的旋转矩阵，表示坐标系之间的纯旋转关系。(正交矩阵：𝑅⊤𝑅=𝐼行列式为 1：det⁡(𝑅)=1)
        cv::Mat dirG = cv::Mat::zeros(3,1,CV_32F);//重力方向的估计值(在经过cvRwg变换前的坐标系下)
        for(vector<KeyFrame*>::iterator itKF = vpKF.begin(); itKF!=vpKF.end(); itKF++)
        {
            if (!(*itKF)->mpImuPreintegrated)//没有预积分，跳过
                continue;
            if (!(*itKF)->mPrevKF)//没有前一帧，跳过
                continue;
            dirG -= (*itKF)->mPrevKF->GetImuRotation()*(*itKF)->mpImuPreintegrated->GetUpdatedDeltaVelocity();//世界坐标系下速度负值累加
            cv::Mat _vel = ((*itKF)->GetImuPosition() - (*itKF)->mPrevKF->GetImuPosition())/(*itKF)->mpImuPreintegrated->dT;//平均速度
            (*itKF)->SetVelocity(_vel);//设置优化前速度初始值
            (*itKF)->mPrevKF->SetVelocity(_vel);//设置优化前速度初始值
        }
        dirG = dirG/cv::norm(dirG);//归一化，只取方向，为什么这个可以作为重力方向的估计值？,因为我们在实际计算中做了一个统计平均（即多帧叠加）如果环境中：相机运动不是很剧烈主要是缓慢移动（甚至停留），那么这些 Δv 中的“其他加速度项”会互相抵消，只有重力项会持续累积。所以我们最终统计出来的 dirG 就“接近”负的重力方向。然后我们就通过归一化：得到重力方向的单位向量。
        cv::Mat gI = (cv::Mat_<float>(3,1) << 0.0f, 0.0f, -1.0f);//重力方向(在经过cvRwg变换后的坐标系下)
        cv::Mat v = gI.cross(dirG);//两个单位向量叉乘，等到垂直于这两个向量的向量v
        const float nv = cv::norm(v);//取模,得到向量长度
        const float cosg = gI.dot(dirG);//两个单位向量点乘，得到夹角的cos
        const float ang = acos(cosg);//gI和dirG的夹角
        cv::Mat vzg = v*ang/nv;//gI到dirG之间的旋转向量
        cvRwg = IMU::ExpSO3(vzg);//指数映射，旋转向量->旋转矩阵
        mRwg = Converter::toMatrix3d(cvRwg);
        mTinit = mpCurrentKeyFrame->mTimeStamp-mFirstTs;
    }
    else//imu已初始化(InitializeIMU在VIBA1和VIBA2中也会调用，那时候imu是已经初始化一次了)
    {
        mRwg = Eigen::Matrix3d::Identity();//直接设为单位阵
        mbg = Converter::toVector3d(mpCurrentKeyFrame->GetGyroBias());
        mba = Converter::toVector3d(mpCurrentKeyFrame->GetAccBias());
    }


mScale=1.0;

mInitTime = mpTracker->mLastFrame.mTimeStamp-vpKF.front()->mTimeStamp;

std::chrono::steady_clock::time_point t0 = std::chrono::steady_clock::now();
Optimizer::InertialOptimization(mpAtlas->GetCurrentMap(), mRwg, mScale, mbg, mba, mbMonocular, infoInertial, false, false, priorG, priorA);
if (mScale<1e-1)
    {
        cout << "scale too small" << endl;
        bInitializing=false;
        return;
    }

    // Before this line we are not changing the map
    {
        unique_lock<mutex> lock(mpAtlas->GetCurrentMap()->mMutexMapUpdate);
        if ((fabs(mScale - 1.f) > 0.00001) || !mbMonocular) {
            Sophus::SE3f Twg(mRwg.cast<float>().transpose(), Eigen::Vector3f::Zero());
            mpAtlas->GetCurrentMap()->ApplyScaledRotation(Twg, mScale, true);//上一步估计出了尺度s和重力方向Rwg，然后矫正位姿、速度和地图点的尺度，并将位姿、速度、地图点进行旋转，使原来的坐标系的负z轴与重力方向重合，这样imu数据变换到world坐标系之后可以直接减去重力g。
            mpTracker->UpdateFrameIMU(mScale, vpKF[0]->GetImuBias(), mpCurrentKeyFrame);
        }

        // Check if initialization OK
        if (!mpAtlas->isImuInitialized())
            for (int i = 0; i < N; i++) {
                KeyFrame *pKF2 = vpKF[i];
                pKF2->bImu = true;
            }
    }
}
void Map::ApplyScaledRotation(const cv::Mat &R, const float s, const bool bScaledVel, const cv::Mat t)
{
    unique_lock<mutex> lock(mMutexMap);
    // Body position (IMU) of first keyframe is fixed to (0,0,0)
    cv::Mat Txw = cv::Mat::eye(4,4,CV_32F);//单位矩阵
    R.copyTo(Txw.rowRange(0,3).colRange(0,3));//R就是Rwg，这是把旋转矩阵 R 拷贝到 Txw 的左上角 3×3 的部分
    cv::Mat Tyx = cv::Mat::eye(4,4,CV_32F);
    cv::Mat Tyw = Tyx*Txw;//Tyw就是世界坐标系到z轴对齐重力方向坐标系的变换矩阵，其实就是施加了Rwg旋转
    Tyw.rowRange(0,3).col(3) = Tyw.rowRange(0,3).col(3)+t;
    cv::Mat Ryw = Tyw.rowRange(0,3).colRange(0,3);
    cv::Mat tyw = Tyw.rowRange(0,3).col(3);
    //遍历所有关键帧，乘上尺度、对齐z轴
    for(set<KeyFrame*>::iterator sit=mspKeyFrames.begin(); sit!=mspKeyFrames.end(); sit++)
    {	
        KeyFrame* pKF = *sit;
        cv::Mat Twc = pKF->GetPoseInverse();//关键帧相机到世界的变换矩阵
        Twc.rowRange(0,3).col(3)*=s;//平移向量乘上尺度
        cv::Mat Tyc = Tyw*Twc;//变换到z轴对齐后的坐标系，现在相当于相机到新世界坐标系的变换
        cv::Mat Tcy = cv::Mat::eye(4,4,CV_32F);//新世界坐标系到相机坐标系的变换
        Tcy.rowRange(0,3).colRange(0,3) = Tyc.rowRange(0,3).colRange(0,3).t();
        Tcy.rowRange(0,3).col(3) = -Tcy.rowRange(0,3).colRange(0,3)*Tyc.rowRange(0,3).col(3);
        pKF->SetPose(Tcy);//更新关键帧位姿
        cv::Mat Vw = pKF->GetVelocity();
        if(!bScaledVel)
            pKF->SetVelocity(Ryw*Vw);
        else
            pKF->SetVelocity(Ryw*Vw*s);//速度也旋转到新的坐标系，再乘上尺度
    }
    for(set<MapPoint*>::iterator sit=mspMapPoints.begin(); sit!=mspMapPoints.end(); sit++)
    {//所有地图点都旋转到新的世界坐标系，并乘上尺度
        MapPoint* pMP = *sit;
        pMP->SetWorldPos(s*Ryw*pMP->GetWorldPos()+tyw);
        pMP->UpdateNormalAndDepth();
    }
    mnMapChange++;//地图改变次数+1
}
```

初始化完成后进行尺度优化

### ScaleRefinement

```c++
void LocalMapping::ScaleRefinement()
{
    if (mbResetRequested)
        return;
    list<KeyFrame*> lpKF;
    KeyFrame* pKF = mpCurrentKeyFrame;
    while(pKF->mPrevKF)
    {
        lpKF.push_front(pKF);
        pKF = pKF->mPrevKF;
    }
    lpKF.push_front(pKF);
    vector<KeyFrame*> vpKF(lpKF.begin(),lpKF.end());
    while(CheckNewKeyFrames())
    {
        ProcessNewKeyFrame();
        vpKF.push_back(mpCurrentKeyFrame);
        lpKF.push_back(mpCurrentKeyFrame);
    }
    const int N = vpKF.size();
    mRwg = Eigen::Matrix3d::Identity();//重力方向初始值
    mScale=1.0;//尺度初始值
    std::chrono::steady_clock::time_point t0 = std::chrono::steady_clock::now();
    //这个函数和前面纯imu初始化的函数同名，但是参数不同，这里只优化尺度和重力方向，其他都设置为固定fixed
    Optimizer::InertialOptimization(mpAtlas->GetCurrentMap(), mRwg, mScale);
    std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
    if (mScale<1e-1) // 1e-1
    {   //尺度太小
        cout << "scale too small" << endl;
        bInitializing=false;
        return;
    }
    // Before this line we are not changing the map
    unique_lock<mutex> lock(mpAtlas->GetCurrentMap()->mMutexMapUpdate);
    std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
    if ((fabs(mScale-1.f)>0.00001)||!mbMonocular)
    {   //利用优化的尺度和重力方向更新关键帧位姿、imu速度、地图点
        mpAtlas->G更新etCurrentMap()->ApplyScaledRotation(Converter::toCvMat(mRwg).t(),mScale,true);
       //更新tracking线程中保存的相对位姿
        mpTracker->UpdateFrameIMU(mScale,mpCurrentKeyFrame->GetImuBias(),mpCurrentKeyFrame);
    }
    std::chrono::steady_clock::time_point t3 = std::chrono::steady_clock::now();
    for(list<KeyFrame*>::iterator lit = mlNewKeyFrames.begin(), lend=mlNewKeyFrames.end(); lit!=lend; lit++)
    {  (*lit)->SetBadFlag();
        delete *lit;
    }
    mlNewKeyFrames.clear();
    double t_inertial_only = std::chrono::duration_cast<std::chrono::duration<double> >(t1 - t0).count();
    mpCurrentKeyFrame->GetMap()->IncreaseChangeIndex();
    return;
}
```

## LoopClosing&Map Merging线程代码

![image-20250423091622340](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250423091622340.png)

![image-20250423091952318](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250423091952318.png)

![image-20250423092357041](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250423092357041.png)

![image-20250423092726615](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250423092726615.png)

![image-20250423093233979](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250423093233979.png)

![image-20250423093827485](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250423093827485.png)

![image-20250424093956225](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250424093956225.png)

![image-20250424094427455](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250424094427455.png)

![image-20250430082850601](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250430082850601.png)

## ORB-slam3+OPEN3D+TSDF

![image-20250506111222872](C:\Users\17274\AppData\Roaming\Typora\typora-user-images\image-20250506111222872.png)